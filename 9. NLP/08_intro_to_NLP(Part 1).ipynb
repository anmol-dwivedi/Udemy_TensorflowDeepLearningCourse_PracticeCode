{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8844db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b891a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU (UUID: GPU-568f2418-97a2-eb4a-4350-001120466be8)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f16f6",
   "metadata": {},
   "source": [
    "# Get Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c758b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-01-12 16:13:59--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: 'helper_functions.py.1'\n",
      "\n",
      "     0K ..........                                            100% 1.07M=0.009s\n",
      "\n",
      "2022-01-12 16:14:00 (1.07 MB/s) - 'helper_functions.py.1' saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669100bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6d35b",
   "metadata": {},
   "source": [
    "# Get a Text Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776936c",
   "metadata": {},
   "source": [
    "Let's start by downloading a text dataset. We'll be using the Real or Not? datset from Kaggle which contains text-based Tweets about natural disasters.\n",
    "\n",
    "The Real Tweets are actually about diasters, for example:\n",
    "\n",
    "- Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "- The Not Real Tweets are Tweets not about diasters (they can be on anything), for example: 'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fcb3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-01-12 16:14:00--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4009:821::2010, 2404:6800:4009:81b::2010, 2404:6800:4009:81a::2010, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4009:821::2010|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: 'nlp_getting_started.zip.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  8% 1.34M 0s\n",
      "    50K .......... .......... .......... .......... .......... 16% 2.55M 0s\n",
      "   100K .......... .......... .......... .......... .......... 25% 5.32M 0s\n",
      "   150K .......... .......... .......... .......... .......... 33% 5.74M 0s\n",
      "   200K .......... .......... .......... .......... .......... 42% 7.37M 0s\n",
      "   250K .......... .......... .......... .......... .......... 50% 6.71M 0s\n",
      "   300K .......... .......... .......... .......... .......... 59% 6.55M 0s\n",
      "   350K .......... .......... .......... .......... .......... 67% 6.91M 0s\n",
      "   400K .......... .......... .......... .......... .......... 75% 4.74M 0s\n",
      "   450K .......... .......... .......... .......... .......... 84% 10.1M 0s\n",
      "   500K .......... .......... .......... .......... .......... 92% 13.7M 0s\n",
      "   550K .......... .......... .......... .......... ...       100% 31.1M=0.1s\n",
      "\n",
      "2022-01-12 16:14:01 (4.75 MB/s) - 'nlp_getting_started.zip.1' saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the dataset from kaggle\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ffdaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742774f4",
   "metadata": {},
   "source": [
    "Unzipping nlp_getting_started.zip gives the following 3 .csv files:\n",
    "\n",
    "- sample_submission.csv - an example of the file you'd submit to the    Kaggle competition of your model's predictions.\n",
    "- train.csv - training samples of real and not real diaster Tweets.\n",
    "- test.csv - testing samples of real and not real diaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7be9c4",
   "metadata": {},
   "source": [
    "# Visualizing A Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb1cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn CSV files into dataframe\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d535dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efdbf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377205a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling the Training DataFrame\n",
    "train_df_shuffled = train_df.sample(frac=1,\n",
    "                                  random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08842bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does test dataframe look like\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4a8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "\n",
      "\n",
      "0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# how many classes are there in the target \n",
    "print(train_df.target.value_counts())\n",
    "print('\\n')\n",
    "print(train_df.target.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d76a74",
   "metadata": {},
   "source": [
    "Since we have two target values, we're dealing with a binary classification problem.\n",
    "\n",
    "It's fairly balanced too, about 60% negative class (target = 0) and 40% positive class (target = 1).\n",
    "\n",
    "Where,\n",
    "- 1 = a real disaster Tweet\n",
    "- 0 = not a real disaster Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6bef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "3263\n"
     ]
    }
   ],
   "source": [
    "# how many total samples\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "799e314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@OriginalFunko @Spencers THUNDER BUDDYS!!!! THUNDER BUDDYS!!!!\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "3 Former Executives to Be Prosecuted in Fukushima Nuclear Disaster http://t.co/JSsmMLNaQ7\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "The 08/06/2015 AlabamaQuake seismic summary w/ #earthquake #news &amp; history http://t.co/zM6VcZqvWk http://t.co/DKNlZNom6n\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "that exploded &amp; brought about the\n",
      "beginning of universe matches what's\n",
      "mentioned in the versethe heaven and Earth\n",
      "(thus the universe)\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/MqbYrAvK6h\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target>0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb334e5",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training Dataset and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551de89",
   "metadata": {},
   "source": [
    "To split our training dataset and create a validation dataset, we'll use Scikit-Learn's **train_test_split()** method and dedicate 10% of the training samples to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a652c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                           train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                           test_size=0.1, # use 10% training dataset for validation,\n",
    "                                                                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac5efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851\n",
      "6851\n",
      "762\n",
      "762\n"
     ]
    }
   ],
   "source": [
    "# Check the lenghts \n",
    "print(len(train_sentences))\n",
    "print(len(train_labels))\n",
    "print(len(val_sentences))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b37ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@mogacola @zamtriossu i screamed after hitting tweet'\n",
      " 'Imagine getting flattened by Kurt Zouma'\n",
      " '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....'\n",
      " \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\"\n",
      " 'Somehow find you and I collide http://t.co/Ee8RpOahPk'\n",
      " '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao'\n",
      " 'destroy the free fandom honestly'\n",
      " 'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE'\n",
      " '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.'\n",
      " 'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt']\n",
      "\n",
      "\n",
      "[0 0 1 0 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# check the first 10 exaples\n",
    "print(train_sentences[:10])\n",
    "print('\\n')\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5246ee",
   "metadata": {},
   "source": [
    "# NLP Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34deae08",
   "metadata": {},
   "source": [
    "### Converting Text into Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b017b",
   "metadata": {},
   "source": [
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "\n",
    "- **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single **token**.\n",
    "  2. **Character-level tokenization**, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single **token**.\n",
    "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n",
    "\n",
    "\n",
    "- **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n",
    "  \n",
    "  1.**Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.\n",
    "  \n",
    "  2.**Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b323650",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95346d1e",
   "metadata": {},
   "source": [
    "Enough talking about tokenization and embeddings, let's create some.\n",
    "\n",
    "We'll practice tokenzation (mapping our words to numbers) first.\n",
    "\n",
    "To tokenize our words, we'll use the helpful preprocessing layer tf.keras.layers.experimental.preprocessing.TextVectorization.\n",
    "\n",
    "The TextVectorization layer takes the following parameters:\n",
    "\n",
    "- max_tokens - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.\n",
    "- standardize - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n",
    "- split - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "- ngrams - How many words to contain per token split, for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "- output_mode - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n",
    "- output_sequence_length - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n",
    "- pad_to_max_tokens - Defaults to False, if True, the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens. Only valid in certain modes, see docs for more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288f3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
    "\n",
    "# use the default TextVectorization parameters\n",
    "text_vectorization = TextVectorization(max_tokens = None, # how many words in the vocabulary (automatically add <OOV>)\n",
    "                                       standardize = \"lower_and_strip_punctuation\",\n",
    "                                       split=\"whitespace\",\n",
    "                                       ngrams= None, # create groups of n-words\n",
    "                                       output_mode = \"int\", # how to map tokens to numbers\n",
    "                                       output_sequence_length = None, # how long do you want your sequences to be\n",
    "                                       # pad_to_max_tokens = True #not valid if using max_tokens = None  \n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba6098",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We've initialized a TextVectorization object with the default settings but let's customize it a little bit for our own use case.\n",
    "\n",
    "In particular, let's set values for max_tokens and output_sequence_length.\n",
    "\n",
    "For max_tokens (the number of words in the vocabulary), multiples of 10,000 (10,000, 20,000, 30,000) or the exact number of unique words in your text (e.g. 32,179) are common values.\n",
    "\n",
    "For our use case, we'll use 10,000.\n",
    "\n",
    "And for the output_sequence_length we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4060767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average number of tokens(words) in the training tweet\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47b626f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dad47034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length of our sequences will be (e.g. how many words from a tweet does a model see)\n",
    "\n",
    "\n",
    "# reinstanciate the textvectorization object\n",
    "text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n",
    "                                   output_mode = \"int\",\n",
    "                                   output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49c3ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Beautiful!\n",
    "\n",
    "To map our TextVectorization instance text_vectorizer to our data, we can call the adapt() method on it whilst passing it our training text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f287297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to training data text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac832e5",
   "metadata": {},
   "source": [
    "Training data mapped! Let's try our text_vectorizer on a custom sentence (one similar to what you might see in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe924b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fedc6",
   "metadata": {},
   "source": [
    "Wonderful, it seems we've got a way to turn our text into numbers (in this case, word-level tokenization). Notice the 0's at the end of the returned tensor, this is because we set output_sequence_length=15, meaning no matter the size of the sequence we pass to text_vectorizer, it always returns a sequence with a length of 15.\n",
    "\n",
    "How about we try our text_vectorizer on a few random sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f15171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " If you're lost and alone or you're sinking like a stone carry onnnn\n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  47,  172,  681,    7,  910,   53,  172,  253,   25,    3, 1756,\n",
       "        2163,    1,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "\\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9a0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1059c6ec",
   "metadata": {},
   "source": [
    "Finally, we can check the unique tokens in our vocabulary using the get_vocabulary() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f63719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in Vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b29cafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [UNK] stands for unknown. it represent the OOV vocabs in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0964b8",
   "metadata": {},
   "source": [
    "# Creating an Embedding using an Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fbe8e8",
   "metadata": {},
   "source": [
    "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
    "\n",
    "The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. 1 = I, 2 = love, 3 = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n",
    "\n",
    "We can see what an embedding of a word looks like by using the tf.keras.layers.Embedding layer.\n",
    "\n",
    "The main parameters we're concerned about here are:\n",
    "\n",
    "- input_dim - The size of the vocabulary (e.g. len(text_vectorizer.get_vocabulary()).\n",
    "- output_dim - The size of the output embedding vector, for example, a value of 100 outputs a feature vector of size 100 for each word.\n",
    "- embeddings_initializer - How to initialize the embeddings matrix, default is \"uniform\" which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "- input_length - Length of sequences being passed to embedding layer.\n",
    "\n",
    "Now, lets create an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bef5d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x140236b6308>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length, # set input shape\n",
    "                             output_dim = 128, # set size of embedding vector\n",
    "                             embeddings_initializer = \"uniform\", # default, initialize randomly\n",
    "                             input_length = max_length, # how long is each input\n",
    "                             name = \"embedding_1\")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6635ad1",
   "metadata": {},
   "source": [
    "Excellent, notice how **embedding** is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns.\n",
    "\n",
    "How about we try it out on a sample sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f8b61",
   "metadata": {},
   "source": [
    "**NOTE** : we cant pass a sentence directly to the embedding layer. the input must be numerical in form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebacf98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " A tin of Tesco dog food 'exploded' and prompted THIS complaint - via @chelsea_dogs #pets #dogs #animals #puppy http://t.co/QzvKPaHsQ7        \n",
      " Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.04284013, -0.01489798, -0.0159496 , ..., -0.01166106,\n",
       "          0.03061062,  0.01972148],\n",
       "        [ 0.04461361,  0.02408149,  0.03878819, ..., -0.02968085,\n",
       "          0.00867941, -0.03825009],\n",
       "        [ 0.00257028, -0.04243337, -0.02343434, ..., -0.00133356,\n",
       "          0.04663134,  0.00035417],\n",
       "        ...,\n",
       "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
       "          0.03332629,  0.02803668],\n",
       "        [-0.01694556, -0.04553232, -0.01517646, ...,  0.02216286,\n",
       "         -0.03468401, -0.00575745],\n",
       "        [-0.02813115,  0.00625932, -0.04300672, ...,  0.02858392,\n",
       "          0.02696348, -0.04848478]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text: \\n {random_sentence}\\\n",
    "        \\n Embedded version:\")\n",
    "\n",
    "# Embed the random sentence (turns an index of numbers into a dense vector of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3db73",
   "metadata": {},
   "source": [
    "**NOTE**: we can see the shape of the above embed output to be (1, 15, 128). The '1' represents the 1 sentence passes to the layer. The 15 represents the output_sequence_lenght of 15 that we defined earlier (Our sentence is broken down into 15 tokens). The '128' represents the embedding for each of the 15 tokens which is 128 in length (i.e. each token gets converted into a vector of 128 length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ece7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.04284013, -0.01489798, -0.0159496 , -0.0226305 ,  0.04298959,\n",
       "        -0.04682324, -0.0026353 ,  0.01123267, -0.03430966, -0.00190909,\n",
       "         0.02867594,  0.0297017 ,  0.02498296,  0.00814937,  0.04493314,\n",
       "         0.04413916, -0.00577633,  0.03141482,  0.00966071, -0.04037346,\n",
       "         0.03765199, -0.01732815, -0.02747819, -0.02993454, -0.02981216,\n",
       "         0.0308927 , -0.02260027, -0.00124929,  0.01732543, -0.02180376,\n",
       "        -0.03130232, -0.04009864,  0.03664006, -0.01028627, -0.03222132,\n",
       "         0.00378202, -0.02535181, -0.00505129,  0.02522682, -0.01333591,\n",
       "         0.0391151 , -0.00091956,  0.02860123, -0.04375963,  0.01296742,\n",
       "         0.0263852 , -0.04896233, -0.04747603,  0.04653648,  0.01485529,\n",
       "        -0.04613405,  0.00209745, -0.00271541,  0.03082445,  0.04200928,\n",
       "        -0.04887832, -0.04972835, -0.0254328 ,  0.03892423, -0.02046248,\n",
       "        -0.0439718 , -0.0345499 , -0.0287706 ,  0.03040506,  0.03975679,\n",
       "         0.0200902 , -0.04209707,  0.03103017, -0.02071968, -0.00967366,\n",
       "        -0.02754457,  0.04369733,  0.01162106,  0.01392407, -0.04973647,\n",
       "        -0.01128978,  0.03097404, -0.03560804, -0.01036394,  0.01256882,\n",
       "         0.02780786, -0.00832926, -0.00847365, -0.03672216,  0.02708429,\n",
       "        -0.03837278,  0.02184938, -0.03841377, -0.0183001 , -0.0102418 ,\n",
       "         0.01095847,  0.0250273 , -0.01542648, -0.01772868, -0.0121302 ,\n",
       "         0.04175656,  0.0218983 ,  0.00327895,  0.02876614,  0.0470233 ,\n",
       "        -0.02171609,  0.03210327, -0.03597981,  0.04386485, -0.02587393,\n",
       "        -0.02878708,  0.00671787, -0.01140868, -0.01845489,  0.00401658,\n",
       "         0.01123898,  0.0443519 , -0.01752155, -0.01664012, -0.01706991,\n",
       "         0.02205701, -0.00104143,  0.039237  , -0.03086319, -0.01964866,\n",
       "        -0.00477686, -0.00302611,  0.0110452 , -0.02808467, -0.0259782 ,\n",
       "        -0.01166106,  0.03061062,  0.01972148], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " \"A tin of Tesco dog food 'exploded' and prompted THIS complaint - via @chelsea_dogs #pets #dogs #animals #puppy http://t.co/QzvKPaHsQ7\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Out a Single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98aca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7bf226",
   "metadata": {},
   "source": [
    "# Modelling a Text Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d056978",
   "metadata": {},
   "source": [
    "Once you've got your inputs and outputs prepared, it's a matter of figuring out which machine learning model to build in between them to bridge the gap.\n",
    "\n",
    "Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n",
    "\n",
    "To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n",
    "\n",
    "More specifically, we'll be building the following:\n",
    "\n",
    "   - Model 0: Naive Bayes (baseline)\n",
    "   - Model 1: Feed-forward neural network (dense model)\n",
    "   - Model 2: LSTM model (RNN)\n",
    "   - Model 3: GRU model (RNN)\n",
    "   - Model 4: Bidirectional-LSTM model (RNN)\n",
    "   - Model 5: 1D Convolutional Neural Network (CNN)\n",
    "   - Model 6: TensorFlow Hub Pretrained Feature Extractor (transfer learning for NLP)\n",
    "   - Model 7: Same as model 6 with 10% of training data\n",
    "\n",
    "Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "\n",
    "   - Construct the model\n",
    "   - Train the model\n",
    "   - Make predictions with the model\n",
    "   - Track prediction evaluation metrics for later comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e02dda",
   "metadata": {},
   "source": [
    "# 1. Model 0 : Naive Bayes Baseline Model (with TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741e905",
   "metadata": {},
   "source": [
    "To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the Multinomial Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96af4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling Pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to number using tfidf\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2075aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline Model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7d6e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using our model\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38305adc",
   "metadata": {},
   "source": [
    "#### Creating an evaluation Function for our Evaluation Metrics (accuracy, precision, recall, F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738a0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"calculates the model accuracy, precision, recall and f1 score of a binary classification model\n",
    "    Args:\n",
    "    -----\n",
    "    y_true = true labels in the form of a 1D array\n",
    "    y_pred = predicted labels in the form of 1D array\n",
    "    \n",
    "    Returns a dictionary of accuracy, precision, recall and f1 score\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    \n",
    "    # calculate the model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred)*100\n",
    "    \n",
    "    # calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true,\n",
    "                                                                                   y_pred,\n",
    "                                                                                  average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                    \"precision\": model_precision,\n",
    "                    \"recall\": model_recall,\n",
    "                    \"f1\": model_f1}\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb62652e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the evaluation metrics for baseline model\n",
    "baseline_results = calculate_results(y_true = val_labels,\n",
    "                                     y_pred = baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d5c41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to Compare 2 model performances\n",
    "\n",
    "\n",
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "    for key, value in baseline_results.items():\n",
    "        print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93b97f",
   "metadata": {},
   "source": [
    "# 2. Model 1: Feed Forward Neural Network (Simple Dense Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9ff20",
   "metadata": {},
   "source": [
    "The first \"deep\" model we're going to build is a single layer dense model. In fact, it's barely going to have a single layer.\n",
    "\n",
    "It'll take our text and labels as input, tokenize the text, create an embedding, find the average of the embedding (using Global Average Pooling) and then pass the average through a fully connected layer with one output unit and a sigmoid activation function.\n",
    "\n",
    "If the previous sentence sounds like a mouthful, it'll make sense when we code it out (remember, if in doubt, code it out).\n",
    "\n",
    "And since we're going to be building a number of TensorFlow deep learning models, we'll import our create_tensorboard_callback() function from helper_functions.py to keep track of the results of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c635b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# create directory to save Tensorboard logs\n",
    "SAVE_DIR = \"model_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81c906da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Dense Model with Functional API\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-D strings\n",
    "x = text_vectorizer(inputs) # turn the inputs text into numbers\n",
    "x = embedding(x) # create an embedding of the numerical inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensonality of the embedding\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid\n",
    "model_1 = tf.keras.Model(inputs, outputs, name = \"model_1_dense\") # construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5c370",
   "metadata": {},
   "source": [
    "Looking good. Our model takes a 1-dimensional string as input (in our case, a Tweet), it then tokenizes the string using text_vectorizer and creates an embedding using embedding.\n",
    "\n",
    "We then (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer\n",
    "\n",
    "Finally, we pass the output of the pooling layer to a dense layer with sigmoid activation (we use sigmoid since our problem is binary classification).\n",
    "\n",
    "Before we can fit our model to the data, we've got to compile it. Since we're working with binary classification, we'll use \"binary_crossentropy\" as our loss function and the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fbd5a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Most of the trainable parameters are contained within the embedding layer. Recall we created an embedding of size 128 (output_dim=128) for a vocabulary of size 10,000 (input_dim=10000), hence the 1,280,000 trainable parameters.\n",
    "\n",
    "Alright, our model is compiled, let's fit it to our training data for 5 epochs. We'll also pass our TensorBoard callback function to make sure our model's training metrics are logged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87e38b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceeef210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_1.compile(loss = \"binary_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c15dde4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/model_1_dense/20220112-161406\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 9ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model_1.history = model_1.fit(train_sentences, # input sentences can be a list of tuples\n",
    "                             train_labels,\n",
    "                             epochs = 5,\n",
    "                             validation_data = (val_sentences, val_labels),\n",
    "                             callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR,\n",
    "                                                                     experiment_name = \"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e67f9314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4766846001148224, 0.787401556968689]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e13a7cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 1)\n"
     ]
    }
   ],
   "source": [
    "# make some predictions and evaluate those\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "\n",
    "print(model_1_pred_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51a9a15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40488204],\n",
       "       [0.7443312 ],\n",
       "       [0.997895  ],\n",
       "       [0.10889999],\n",
       "       [0.11143529],\n",
       "       [0.93556094],\n",
       "       [0.9134595 ],\n",
       "       [0.9925345 ],\n",
       "       [0.97156817],\n",
       "       [0.2657034 ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 10 predictions\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad1f1946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ae33b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model_1 performance metrics\n",
    "model_1_results = calculate_results(y_true = val_labels,\n",
    "                                   y_pred= model_1_preds)\n",
    "\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "919b85ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a08eee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d13e248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671ddc8",
   "metadata": {},
   "source": [
    "we can see that our dense model is actually performing worse than our baseline Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a989a",
   "metadata": {},
   "source": [
    "### Visulizing Model's Learned Embeddings with Tensorflow's Projector Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5dcc66c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is', 'for', 'on', 'you', 'my', 'with', 'it', 'that', 'at', 'by', 'this', 'from', 'be', 'are', 'was', 'have', 'like', 'as', 'up', 'so', 'just', 'but', 'me', 'im', 'your', 'not', 'amp', 'out', 'its', 'will', 'an', 'no', 'has', 'fire', 'after', 'all', 'when', 'we', 'if', 'now', 'via', 'new', 'more', 'get', 'or', 'about', 'what', 'he', 'people', 'news', 'been', 'over', 'one', 'how', 'dont', 'they', 'who', 'into', 'were', 'do', 'us', '2', 'can', 'video', 'emergency', 'there', 'disaster', 'than', 'police', 'would', 'his', 'still', 'her', 'some', 'body', 'storm', 'crash', 'burning', 'suicide', 'back', 'man', 'california', 'why', 'time', 'them', 'had', 'buildings', 'rt', 'first', 'cant', 'see', 'got', 'day', 'off', 'our', 'going', 'nuclear', 'know', 'world', 'bomb', 'fires', 'love', 'killed', 'go', 'attack', 'youtube', 'dead', 'two', 'families', '3', 'train', 'full', 'being', 'war', 'many', 'today', 'think', 'only', 'car', 'accident', 'life', 'hiroshima', 'their', 'say', 'may', 'down', 'watch', 'good', 'could', 'want', 'last', 'here', 'years', 'u', 'then', 'make', 'did', 'wildfire', 'way', 'help', 'best', 'too', 'even', 'because', 'home', 'death', 'collapse', 'bombing', 'mass', 'him', 'black', 'am', 'those', 'need', 'fatal', 'army', 'another', 'work', 'take', 'should', 'really', 'please', 'mh370', 'youre', 'look', 'lol', 'hot', 'pm', 'legionnaires', '4', 'right', '5', 'let', 'city', 'year', 'wreck', 'school', 'northern', 'much', 'forest', 'bomber', 'water', 'she', 'never', 'read', 'latest', 'homes', 'great', 'every', '1', 'live', 'god', 'fear', 'any', '\\x89Û', 'under', 'said', 'old', 'floods', '2015', 'getting', 'atomic', 'while', 'top', 'obama', 'feel', 'thats', 'since', 'near', 'flames', 'ever', 'come', 'where', 'these', 'military', 'japan', 'found', 'content', 'ass', 'without', 'weather', 'most', 'flooding', 'flood', 'damage', 'which', 'shit', 's', 'hope', 'everyone', 'before', 'stop', 'plan', 'malaysia', 'injured', 'hit', 'evacuation', 'during', 'debris', 'cross', 'coming', 'wild', 'well', 'times', 'sinking', 'oil', 'fucking', 'check', 'cause', 'weapons', 'truck', 'food', 'bloody', 'always', 'weapon', 'theres', 'state', 'little', 'injuries', 'free', 'wounded', 'summer', 'smoke', 'severe', 'reddit', 'next', 'movie', 'ive', 'hes', 'fall', 'evacuate', 'confirmed', 'bad', 'again', 'thunderstorm', 'set', 'night', 'natural', 'looks', 'heat', 'face', 'earthquake', 'boy', 'whole', 'until', 'thunder', 'through', 'says', 'panic', 'outbreak', 'made', 'lightning', 'fatalities', 'family', 'explosion', 'end', 'destroy', 'derailment', 'air', 'w', 'terrorist', 'survive', 'screaming', 'saudi', 'refugees', 'rain', 'murder', 'loud', 'liked', 'house', 'gonna', 'failure', 'collided', 'bag', 'attacked', 'ambulance', '70', 'wind', 'services', 'save', 'report', 'migrants', 'head', 'explode', 'charged', 'change', 'big', 'also', 'wrecked', 'warning', 'update', 'run', 'rescuers', 'released', 'photo', 'massacre', 'injury', 'hurricane', 'high', 'hail', 'fuck', 'does', 'destroyed', 'bus', 'blood', '40', '\\x89ÛÒ', 'wreckage', 'violent', 'twister', 'trauma', 'tragedy', 'terrorism', 'survivors', 'survived', 'sinkhole', 'sandstorm', 'road', 'rioting', 'red', 'real', 'put', 'post', 'national', 'missing', 'landslide', 'keep', 'girl', 'drought', 'curfew', 'breaking', 'bags', 'white', 'twitter', 'tonight', 'structural', 'spill', 'service', 'screamed', 'rescued', 'rescue', 'phone', 'ok', 'oh', 'mosque', 'lives', 'horrible', 'harm', 'game', 'dust', 'destruction', 'deluge', 'deaths', 'crashed', 'cliff', 'catastrophe', 'boat', 'away', 'august', 'area', 'apocalypse', 'woman', 'whirlwind', 'traumatised', 'stock', 'saw', 'ruin', 'riot', 'quarantine', 'kills', 'island', 'investigators', 'ill', 'hostages', 'hazard', 'danger', 'call', '15', 'women', 'windstorm', 'things', 'suspect', 'show', 'reunion', 'quarantined', 'lava', 'heart', 'engulfed', 'detonate', 'crush', 'collapsed', 'came', 'better', 'battle', 'armageddon', 'airplane', 'against', 'affected', 'use', 'trapped', 'thank', 'sunk', 'story', 'send', 'part', 'other', 'must', 'mudslide', 'market', 'iran', 'famine', 'exploded', 'electrocuted', 'ebay', 'displaced', 'derailed', 'derail', 'burned', 'bombed', 'blown', 'baby', 'around', 'zone', 'wave', 'wanna', 'sure', 'someone', 'screams', 'razed', 'power', 'obliterated', 'long', 'land', 'hundreds', 'heard', 'group', 'flattened', 'drown', 'doing', 'care', 'bridge', 'bagging', '9', 'went', 'used', 'typhoon', 'trouble', 'tornado', 'thought', 'thing', 'river', 'responders', 'past', 'pandemonium', 'officials', 'meltdown', 'lot', 'least', 'inundated', 'id', 'hostage', 'hijacking', 'hazardous', 'goes', 'drowning', 'didnt', 'devastation', 'demolish', 'collide', 'casualties', 'calgary', 'bang', 'anniversary', 'yet', 'wounds', 'volcano', 'tsunami', 'sue', 'st', 'song', 'something', 'shoulder', 'security', 'prebreak', 'possible', 'pkk', 'panicking', 'obliteration', 'obliterate', 'murderer', 'minute', 'light', 'lets', 'kill', 'isis', 'india', 'hijacker', 'hellfire', 'government', 'few', 'evacuated', 'due', 'detonated', 'desolation', 'crushed', 'chemical', 'blew', 'blazing', 'blast', 'annihilated', 'airport', '6', 'week', 'upheaval', 'trying', 'three', 'thanks', 'sound', 'soon', 'sirens', 'rainstorm', 'plane', 'music', 'making', 'kids', 'issues', 'half', 'guys', 'fedex', 'done', 'died', 'detonation', 'days', 'cyclone', 'county', 'collision', 'caused', 'catastrophic', 'bleeding', 'beautiful', '8', 'words', 'very', 'traffic', 'south', 'remember', 'policy', 'place', 'nothing', 'north', 'mp', 'longer', 'left', 'israeli', 'hell', 'fun', 'drowned', 'demolished', 'cool', 'both', 'bioterror', 'believe', 'avalanche', 'arson', 'turkey', 'snowstorm', 'site', 'shot', 'shooting', 'pic', 'nowplaying', 'media', 'islam', 'inside', 'hijack', 'helicopter', 'fight', 'fatality', 'fan', 'electrocute', 'doesnt', 'building', 'brown', 'bc', 'actually', '16yr', 'yes', 'watching', 'wait', 'ur', 'tell', 'swallowed', 'seismic', 'second', 'rubble', 're\\x89Û', 'plans', 'men', 'memories', 'line', 'la', 'horror', 'health', 'having', 'find', 'eyewitness', 'deluged', 'children', 'bush', 'anything', 'already', 'almost', 'aircraft', 'yourself', 'yeah', 'whats', 'tomorrow', 'such', 'start', 'side', 'searching', 'saved', 'reactor', 'probably', 'play', 'person', 'peace', 'outside', 'officer', 'nearby', 'n', 'maybe', 'lost', 'literally', 'hours', 'hear', 'far', 'die', 'demolition', 'data', 'crews', 'conclusively', 'business', 'american', '20', '\\x89ÛÓ', 'west', 'waves', 'team', 'street', 'stay', 'soudelor', 'reuters', 'manslaughter', 'leather', 'job', 'history', 'hey', 'feeling', 'eyes', 'everything', 'declares', 'deal', 'casualty', 'bodies', 'amid', 'ablaze', '7', '50', '30', '12', 'youth', 'wont', 'wake', 'theyre', 'support', 'stretcher', 'same', 'rise', 'picking', 'photos', 'own', 'others', 'order', 'omg', 'okay', 'name', 'myself', 'money', 'makes', 'leave', 'lab', 'gt', 'gets', 'flag', 'desolate', 'crisis', 'center', 'book', 'blight', 'blaze', 'ago', 'abc', '11yearold', 'womens', 'typhoondevastated', 'tv', 'trench', 'trains', 'texas', 'space', 'siren', 'shes', 'self', 'saipan', 'reason', 'rd', 'pretty', 'pick', 'offensive', 'move', 'meek', 'major', 'm', 'low', 'lord', 'huge', 'hat', 'flash', 'feared', 'fast', 'effect', 'course', 'country', 'control', 'class', 'child', 'chance', 'caught', 'called', 'bioterrorism', 'bestnaijamade', 'become', 'bar', 'banned', 'ball', 'aug', 'annihilation', 'wrong', 'win', 'usa', 'united', 'town', 'totally', 'toddler', 'though', 'temple', 'taken', 'stand', 'spot', 'signs', 'ship', 'pakistan', 'online', 'level', 'ladies', 'jobs', 'isnt', 'happy', 'hailstorm', 'friends', 'disea', 'damn', 'couple', 'case', 'blue', 'bigger', 'america', 'across', '10', 'yours', 'village', 'try', 'transport', 'talk', 'seen', 'russian', 'radio', 'projected', 'once', 'official', 'needs', 'nearly', 'mount', 'might', 'mayhem', 'instead', 'hollywood', 'haha', 'guy', 'gun', 'green', 'front', 'finally', 'favorite', 'experts', 'entire', 'east', 'daily', 'crazy', 'computers', 'coaches', 'christian', 'china', 'blizzard', 'anyone', 'aint', 'action', '25', 'virgin', 'vehicle', 'truth', 'trust', 'takes', 't', 'star', 'sorry', 'running', 'refugio', 'reddits', 'poor', 'pain', 'mom', 'miners', 'marks', 'looking', 'knock', 'issued', 'insurance', 'ignition', 'houses', 'heavy', 'hate', 'hard', 'happened', 'global', 'giant', 'gbbo', 'flight', 'eye', 'emmerdale', 'driver', 'devastated', 'd', 'costlier', 'cnn', 'cars', 'camp', 'beach', 'arsonist', 'angry', 'alone', 'added', '05', 'york', 'wonder', 'uk', 'turn', 'taking', 'subreddits', 'sounds', 'scared', 'russia', 'rly', 'reports', 'ready', 'quiz', 'public', 'property', 'pradesh', 'ppl', 'playing', 'pay', 'parole', 'pamela', 'pakistani', 'outrage', 'niggas', 'nagasaki', 'myanmar', 'muslims', 'mop', 'madhya', 'mad', 'lmao', 'learn', 'large', 'govt', 'give', 'gems', 'gave', 'funtenna', 'fukushima', 'former', 'film', 'earth', 'drive', 'downtown', 'dog', 'comes', 'closed', 'cake', 'british', 'bring', 'bbc', 'b', 'appears', 'aftershock', '13', '11', 'young', 'wow', 'worst', 'waving', 'washington', 'wanted', 'vs', 'view', 'upon', 'tweet', 'tree', 'tote', 'thousands', 'thinking', 'theater', 'soul', 'sky', 'sign', 'shows', 'shift', 'seeing', 'sea', 'scene', 'safety', 'rules', 'rock', 'reported', 'r', 'pray', 'playlist', 'patience', 'passengers', 'park', 'nws', 'nigerian', 'morning', 'moment', 'mode', 'listen', 'likely', 'libya', 'led', 'israel', 'human', 'hiring', 'handbag', 'hand', 'gop', 'geller', 'gas', 'galactic', 'friend', 'france', 'following', 'follow', 'fashion', 'enough', 'else', 'driving', 'drake', 'don\\x89Ûªt', 'disease', 'declaration', 'cut', 'colorado', 'chinas', 'chile', 'centre', 'businesses', 'biggest', 'behind', 'bed', 'bayelsa', 'awesome', 'arrested', 'apollo', 'ancient', '70th', '100', 'x', 'working', 'wednesday', 'wasnt', 'unconfirmed', 'twelve', 'turned', 'tried', 'thursday', 'terror', 'super', 'sex', 'secret', 'sad', 'risk', 'recount', 'problem', 'potus', 'planned', 'parents', 'occurred', 'number', 'neighbours', 'mph', 'militants', 'middle', 'link', 'landing', 'lamp', 'info', 'idea', 'holding', 'governor', 'glad', 'germs', 'firefighters', 'escape', 'ebola', 'early', 'dude', 'deep', 'date', 'cree', 'claims', 'cdt', 'break', 'ave', 'art', 'anthrax', '60', '16', '12000', 'worse', 'worlds', 'western', 'wants', 'wall', 'walk', 'using', 'true', 'till', 'started', 'spring', 'small', 'silver', 'serious', 'provoke', 'played', 'party', 'nice', 'mountain', 'months', 'mod', 'miss', 'mishaps', 'metal', 'med', 'lies', 'lake', 'lady', 'japanese', 'involving', 'investigating', 'internet', 'hour', 'helping', 'havent', 'gtgt', 'funny', 'force', 'financial', 'faux', 'falling', 'expected', 'download', 'direction', 'department', 'denver', 'delivers', 'crematoria', 'coast', 'climate', 'ca', 'buy', 'bombs', 'album', 'alarm', 'ahead', 'account', 'aba', '\\x89ÛÏwhen', 'youll', 'wish', 'wedding', 'victims', 'udhampur', 'turkish', 'travel', 'took', 'told', 'together', 'todays', 'threat', 'sun', 'stories', 'sick', 'shots', 'share', 'series', 'sense', 'season', 'saying', 'salt', 'room', 'record', 'purse', 'prepare', 'plunging', 'philippines', 'pass', 'p', 'offroad', 'nigga', 'members', 'means', 'louis', 'london', 'living', 'linked', 'king', 'join', 'international', 'information', 'horse', 'hasnt', 'happening', 'guide', 'guess', 'ground', 'future', 'friday', 'four', 'feat', 'fact', 'expect', 'etc', 'door', 'different', 'dies', 'de', 'company', 'comment', 'causes', 'career', 'cameroon', 'begin', 'australia', 'arent', 'answer', 'along', 'allows', 'alabama', 'airlines', 'act', '600', '2014', '17', 'wtf', 'worry', 'wife', 'weekend', 'walking', 'victim', 'v', 'utc20150805', 'trump', 'tho', 'text', 'test', 'teen', 'taiwan', 'tablet', 'syrian', 'strike', 'straight', 'states', 'short', 'ships', 'shape', 'setting', 'sensorsenso', 'search', 'rocky', 'research', 'reading', 'rather', 'rate', 'press', 'piece', 'phoenix', 'orders', 'ones', 'nyc', 'minutes', 'million', 'michael', 'mean', 'map', 'lucky', 'killing', 'jonathan', 'islamic', 'internally', 'interesting', 'indian', 'hero', 'happens', 'happen', 'hair', 'giving', 'girls', 'general', 'freak', 'fighting', 'felt', 'feels', 'fat', 'fans', 'episode', 'enugu', 'effects', 'each', 'e', 'drunk', 'double', 'dogs', 'destroys', 'desires', 'dc', 'dance', 'dan', 'cover', 'court', 'complete', 'cold', 'close', 'changes', 'carrying', 'captures', 'cable', 'buses', 'brooklyn', 'board', 'blocked', 'bit', 'between', 'ban', 'ashes', 'arianagrande', 'apc', 'animal', 'amazon', 'alive', 'agree', 'africa', 'absolutely', '2nd', '24', '2013', '1st', '19', '18', '0', 'yall', 'wrought', 'worth', 'winds', 'vote', 'version', 'upset', 'un', 'trent', 'trees', 'training', 'tracks', 'total', 'technology', 'suspected', 'strong', 'standard', 'stadium', 'specially', 'special', 'sometimes', 'soldiers', 'snow', 'smaug', 'sleeping', 'single', 'sh', 'selfimage', 'route', 'ross', 'roosevelt', 'response', 'repatriated', 'remove', 'radiation', 'queen', 'program', 'prevent', 'population', 'picture', 'petition', 'palestinian', 'office', 'nc', 'navy', 'moving', 'millions', 'metro', 'meet', 'mediterranean', 'marians', 'leaving', 'lead', 'late', 'kit', 'killer', 'image', 'hold', 'hobo', 'hello', 'handbags', 'gunman', 'grows', 'gotta', 'gone', 'gold', 'games', 'fully', 'french', 'foxnews', 'feet', 'express', 'examining', 'event', 'evening', 'either', 'dr', 'dangerous', 'dad', 'currently', 'cranes', 'cops', 'colour', 'chicago', 'central', 'causing', 'cat', 'cannot', 'canada', 'calls', 'brother', 'broke', 'box', 'beyond', 'beat', 'based', 'avoid', 'anymore', 'afghanistan', 'able', 'abandoned', '22', '1980', '14', '06', 'yyc', 'wwii', 'word', 'won', 'woke', 'videos', 'usgs', 'usatoday', 'updates', 'tribal', 'trfc', 'totaling', 'thunderstorms', 'tension', 'tears', 'target', 'talking', 'supposed', 'sunday', 'stuff', 'students', 'struggles', 'starts', 'stage', 'squad', 'spos', 'spaceship', 'son', 'sitting', 'sismo', 'shut', 'shouldnt', 'shall', 'seeks', 'seek', 'screen', 'saving', 'san', 'runs', 'relief', 'region', 'reasons', 're', 'question', 'president', 'point', 'pile', 'perfect', 'percent', 'pdp', 'paul', 'open', 'municipal', 'motorcyclist', 'month', 'modified', 'match', 'main', 'lots', 'lose', 'lie', 'less', 'leading', 'later', 'kinda', 'keeps', 'insurer', 'industry', 'imagine', 'ice', 'hunt', 'hospital', 'honestly', 'hilarious', 'ft', 'forever', 'forecast', 'football', 'fleets', 'five', 'fears', 'extreme', 'everywhere', 'emotional', 'drink', 'dream', 'damaged', 'da', 'cruz', 'crime', 'create', 'cook', 'confirms', 'completely', 'community', 'combo', 'coffee', 'cancer', 'c130', 'brought', 'broken', 'boys', 'books', 'block', 'began', 'bay', 'babies', 'attacks', 'ask', 'apply', 'ap', 'antioch', 'among', 'allah', 'according', '911', '4000', '31', '2011', 'yo', 'worldnews', 'wmata', 'wired', 'welcome', 'watched', 'wars', 'waiting', 'usually', 'units', 'understand', 'tropical', 'tram', 'tonto', 'tips', 'time20150806', 'tickets', 'thx', 'terrorists', 'system', 'streets', 'stream', 'stopped', 'sports', 'spent', 'sir', 'simple', 'signed', 'shower', 'shares', 'shame', 'sent', 'seems', 'secrets', 'seconds', 'seattle', 'saturday', 'satellite', 'rule', 'round', 'rip', 'ridge', 'richmond', 'review', 'return', 'repair', 'rea\\x89Û', 'quite', 'pressure', 'practice', 'potential', 'pool', 'plus', 'pilot', 'pics', 'parenthood', 'owner', 'outlook', 'original', 'opens', 'opening', 'onto', 'oklahoma', 'ocean', 'o784', 'o', 'nursing', 'nah', 'mortal', 'mix', 'mine', 'mention', 'mediterran', 'matter', 'massive', 'mary', 'lt', 'loving', 'loved', 'list', 'limited', 'lil', 'leads', 'leader', 'kombat', 'kid', 'kick', 'justinbieber', 'issue', 'irandeal', 'inner', 'incident', 'impact', 'humanity', 'hobbit', 'hits', 'himself', 'highway', 'hi', 'heres', 'hawaii', 'hands', 'guns', 'grill', 'greatest', 'gov', 'gm', 'gay', 'forget', 'firefighter', 'fine', 'final', 'false', 'facebook', 'faan', 'exploration', 'exit', 'exchanging', 'epic', 'el', 'edm', 'earlier', 'drinking', 'disco', 'directioners', 'deadly', 'david', 'dark', 'crying', 'cost', 'continue', 'concerned', 'companies', 'common', 'civilians', 'civilian', 'character', 'cases', 'careful', 'canyon', 'calling', 'burn', 'build', 'browser', 'bro', 'bringing', 'bought', 'blog', 'birthday', 'bin', 'bicyclist', 'ben', 'battlefield', 'arrived', 'ar', 'amazing', 'afternoon', 'acres', 'accidents', 'abstorm', '90', '500', '26', 'you\\x89Ûªve', 'youve', 'yesterday', 'wx', 'writing', 'worked', 'within', 'wildfires', 'weird', 'ways', 'warnings', 'wa', 'vuitton', 'voice', 'visit', 'vietnam', 'unless', 'universe', 'trip', 'township', 'tongue', 'toilet', 'thomas', 'tent', 'tank', 'syria', 'survival', 'stupid', 'stone', 'steve', 'starting', 'sport', 'somalia', 'social', 'smh', 'sleep', 'situation', 'shoes', 'seven', 'sensor', 'sees', 'seat', 'science', 'sadly', 'rubber', 'roof', 'roads', 'result', 'respond', 'replace', 'remains', 'related', 'reduce', 'recovery', 'realise', 'rare', 'rail', 'quote', 'quickly', 'quick', 'pussy', 'protect', 'problems', 'price', 'preparedness', 'powerful', 'posts', 'portland', 'pop', 'political', 'points', 'player', 'photography', 'pathogens', 'officers', 'none', 'needed', 'nature', 'mumbai', 'ms', 'mr', 'moments', 'missed', 'minecraft', 'mill', 'mile', 'md', 'mayan', 'manchester', 'malaysian', 'loss', 'loose', 'listening', 'lion', 'linkury', 'lights', 'lifted', 'laden', 'knew', 'kindle', 'jeb', 'it\\x89Ûªs', 'italian', 'iraq', 'invoices', 'inundation', 'inj', 'including', 'illegal', 'i5', 'hwy', 'hwo', 'httptcoqew4c5m1xd', 'hill', 'hearing', 'headed', 'ha', 'guardian', 'gods', 'glass', 'genocide', 'gaza', 'freakiest', 'fort', 'forces', 'fog', 'fish', 'feminists', 'fell', 'fantasy', 'falls', 'f', 'experience', 'exchange', 'except', 'equipment', 'epicentre', 'ep', 'ended', 'em', 'economy', 'economic', 'dropped', 'dozens', 'disney', 'discovered', 'di', 'created', 'continues', 'conference', 'computer', 'clutch', 'church', 'choice', 'china\\x89Ûªs', 'chief', 'cheese', 'charity', 'canaanites', 'burns', 'bruh', 'broadway', 'brain', 'bout', 'bombings', 'beyhive', 'bet', 'baseball', 'bank', 'automatic', 'audio', 'attention', 'asking', 'asked', 'articles', 'areas', 'arabia', 'angel', 'al', 'ah', 'add', 'access', '4x4', '3g', '1000', 'zombie', 'yrs', 'ya', 'xd', 'write', 'wouldnt', 'wins', 'wings', 'window', 'wheavenly', 'weeks', 'wed', 'waste', 'vinyl', 'vine', 'veterans', 'van', 'valley', 'utc', 'updated', 'unlocked', 'unit', 'type', 'twice', 'tweets', 'tube', 'troops', 'triggered', 'trailer', 'tour', 'tough', 'touch', 'toronto', 'tom', 'toll', 'tired', 'threatens', 'thoughts', 'theatre', 'terrible', 'ten', 'tcot', 'swallows', 'sw', 'surrounded', 'surprise', 'sunset', 'subject', 'style', 'struck', 'storms', 'store', 'stops', 'steps', 'steel', 'stealing', 'stars', 'standuser', 'sparked', 'soundcloud', 'somebody', 'society', 'snap', 'slightly', 'skin', 'size', 'sit', 'sinjar', 'sidelines', 'sicily', 'shelter', 'shadow', 'several', 'september', 'senior', 'senator', 'se', 'scary', 'satchel', 'santa', 'salem', 'sale', 'safe', 'runway', 'row', 'root', 'rocks', 'rn', 'rising', 'riots', 'residents', 'republicans', 'reminds', 'remembering', 'releases', 'release', 'relatives', 'ran', 'quran', 'questions', 'putin', 'prosecuted', 'prophetmuhammad', 'project', 'prime', 'powerlines', 'poll', 'pipe', 'pilots', 'pc', 'patient', 'passing', 'parleys', 'parker', 'page', 'operation', 'officially', 'nytimes', 'ny', 'notices', 'nine', 'nashville', 'murdered', 'mt', 'movies', 'moon', 'mood', 'monogram', 'mma', 'mission', 'minister', 'mining', 'mini', 'mind', 'mikeparractor', 'mets', 'message', 'mentions', 'meeting', 'medical', 'meatloving', 'maximum', 'max', 'markets', 'maintenance', 'madinah', 'mac', 'lt3', 'lovely', 'loop', 'local', 'library', 'levels', 'legacy', 'learning', 'laws', 'lane', 'lack', 'knows', 'knee', 'kisii', 'kidnapped', 'john', 'joe', 'jackson', 'jack', 'i\\x89Ûªm', 'itunes', 'interview', 'intensity', 'indeed', 'incredible', 'ii', 'ie', 'ideas', 'hurts', 'hunters', 'humans', 'httptcoq2eblokeve', 'httptcoencmhz6y34', 'homeless', 'heroes', 'held', 'heads', 'h', 'gunfire', 'gta', 'gotten', 'given', 'girlfriend', 'german', 'fruit', 'fr', 'foxtrot', 'forced', 'followers', 'focus', 'florida', 'floor', 'flat', 'fix', 'fired', 'female', 'feed', 'failed', 'fail', 'exp', 'executives', 'evil', 'everyday', 'estimate', 'entertainment', 'enjoy', 'england', 'enemy', 'electronic', 'edt', 'edition', 'eat', 'dying', 'dubstep', 'drone', 'drill', 'dollar', 'documents', 'dnb', 'distance', 'destiny', 'defense', 'debate', 'cuz', 'cute', 'curved', 'criminals', 'crap', 'cramer', 'couldnt', 'costs', 'cop', 'consider', 'congress', 'conditions', 'concert', 'complex', 'co', 'club', 'click', 'chinese', 'charging', 'ceo', 'carry', 'card', 'capsizes', 'can\\x89Ûªt', 'calories', 'calm', 'calif', 'c', 'by\\x89Û', 'button', 'butter', 'burst', 'budget', 'broad', 'brazil', 'boston', 'blvd', 'blk', 'blessings', 'blessed', 'blamed', 'bells', 'bear', 'awful', 'avoiding', 'authorities', 'australian', 'associated', 'arrive', 'arms', 'apparently', 'app', 'animalrescue', 'amsterdam', 'aka', 'aim', 'af', 'advisory', 'adult', 'actions', 'accidentally', 'above', '5km', '53inch', '4wd', '35', '300w', '1945', 'zionist', 'z10', 'youngheroesid', 'youd', 'yobe', 'yay', 'xbox', 'ww1', 'written', 'wouldve', 'worried', 'workers', 'wondering', 'wonderful', 'wo', 'williams', 'whos', 'whether', 'wet', 'weed', 'wearing', 'wealth', 'warship', 'warns', 'viralspell', 'violence', 'vehicles', 'usual', 'users', 'upper', 'unveiled', 'ultimate', 'ugh', 'twia', 'tryna', 'truly', 'treatment', 'treat', 'trapmusic', 'trap', 'traditional', 'track', 'towel', 'toward', 'tornadoes', 'title', 'thus', 'throwing', 'thriller', 'threatening', 'theyll', 'tennessee', 'tempered', 'ted', 'tech', 'teams', 'tea', 'tbt', 'tax', 'tag', 'sydney', 'swim', 'sweet', 'surprised', 'stuart', 'structures', 'structure', 'streak', 'strategy', 'strange', 'stranded', 'status', 'statement', 'staff', 'springs', 'spirit', 'spider', 'spanish', 'somehow', 'solution', 'solar', 'socialnews', 'smoking', 'sister', 'singing', 'sing', 'shop', 'shipping', 'shepherd', 'sharp', 'sexual', 'served', 'seriously', 'serial', 'sentinel', 'sarah', 'sac', 'rÌ©union', 'romance', 'roll', 'revealed', 'results', 'restore', 'rest', 'responsible', 'responds', 'respect', 'residential', 'reporting', 'reno', 'remain', 'register', 'reduced', 'rear', 'realized', 'read\\x89Û', 'ray', 'rape', 'rally', 'rains', 'quest', 'quality', 'putting', 'purple', 'pump', 'protector', 'prophet', 'profile', 'prices', 'pres', 'pov', 'port', 'politics', 'plug', 'plot', 'playoffs', 'planet', 'plains', 'plague', 'places', 'piling', 'picked', 'physical', 'philly', 'persons', 'personal', 'peanut', 'patrick', 'parts', 'paper', 'pantherattack', 'pak', 'owners', 'option', 'okwx', 'of\\x89Û', 'offer', 'numbers', 'nuke', 'normal', 'newest', 'nd', 'naved', 'nation', 'nasahurricane', 'named', 'museum', 'murderous', 'moves', 'moved', 'motor', 'mo', 'mlb', 'miles', 'migrant', 'mido', 'microlight', 'mhtw4fnet', 'mess', 'memory', 'memorial', 'maria', 'mansehra', 'manager', 'management', 'mail', 'lungs', 'luck', 'lowndes', 'lower', 'loves', 'lorries', 'looting', 'looked', 'let\\x89Ûªs', 'legal', 'law', 'laughing', 'larger', 'justice', 'july', 'judge', 'jewish', 'jet', 'jamaica', 'jam', 'jacksonville', 'itself', 'italy', 'ir', 'iphone', 'insane', 'injuryi495', 'increased', 'iii', 'igers', 'ices\\x89Û', 'icemoon', 'i77', 'hungry', 'httptcovvplfqv58p', 'httptcoksawlyux02', 'housing', 'hotel', 'hopefully', 'hip', 'hearts', 'hd', 'guillermo', 'gtgtgt', 'grow', 'grey', 'grade', 'grace', 'google', 'golf', 'goal', 'germany', 'george', 'generation', 'gang', 'gabon', 'fresh', 'freedom', 'fox', 'finnish', 'fighters', 'field', 'festival', 'fav', 'fake', 'faced', 'extremely', 'extra', 'external', 'explosionproof', 'experiments', 'exactly', 'everyones', 'estimated', 'especially', 'eruption', 'error', 'energy', 'ends', 'ems', 'elephant', 'electrical', 'effort', 'education', 'easy', 'dvd', 'duty', 'dutch', 'drug', 'drop', 'drones', 'drawn', 'dramatic', 'djicemoon', 'diving', 'disrupts', 'disneys', 'diet', 'details', 'depth', 'demand', 'degrees', 'decided', 'debt', 'deals', 'darude', 'dare', 'damages', 'cry', 'crossed', 'cream', 'covers', 'covered', 'countries', 'copilot', 'considering', 'conflict', 'confirm', 'combined', 'collisionno', 'collection', 'clip', 'clear', 'clean', 'classic', 'civil', 'cities', 'cinema', 'cia', 'christmas', 'christians', 'charge', 'characters', 'certain', 'campaign', 'built', 'buffalo', 'bride', 'brakes', 'bowl', 'boss', 'bob', 'blind', 'blackberry', 'bjp', 'bitch', 'be\\x89Û', 'below', 'beginning', 'becomes', 'became', 'beam', 'bb17', 'bat', 'bargain', 'b4', 'australias', 'asap', 'article', 'approaches', 'anyway', 'anti', 'annual', 'animals', 'anchorage', 'amongst', 'americans', 'alps', 'allow', 'aid', 'age', 'afterlife', 'afraid', 'afghan', 'advance', 'address', 'acts', 'activity', 'activated', 'abuse', 'abcnews', '97georgia', '731', '3d', '33', '320', '300', '101', 'åÊ', '\\x89Û÷politics', '\\x89ÛÏthe', '\\x89ÛÏa', 'zouma', 'z', 'yr', 'yazidis', 'y', 'xp', 'wy', 'writer', 'wound', 'worstsummerjob', 'wood', 'wither', 'wit', 'wireless', 'wire', 'winston', 'windows', 'wheres', 'whao', 'weve', 'werent', 'website', 'wash', 'warfighting', 'wanting', 'walmart', 'waimate', 'vulnerable', 'votejkt48id', 'vintage', 'vets', 'values', 'val', 'vacant', 'va', 'utterly', 'utter', 'uses', 'usagov', 'uribe', 'unsuckdcmetro', 'unlocking', 'university', 'union', 'understanding', 'ugly', 'uganda', 'twin', 'tutorial', 'turns', 'turbine', 'tubestrike', 'trucks', 'trolley', 'tripledigit', 'tribune', 'trial', 'transit', 'tracking', 'tools', 'tons', 'todd', 'time20150805', 'throw', 'thousand', 'theyve', 'theory', 'themselves', 'th', 'temporary300', 'telling', 'taste', 'targeting', 'tampa', 'table', 'swimming', 'swansea', 'supreme', 'summerfate', 'suffer', 'sucks', 'success', 'subs', 'sub', 'study', 'stress', 'strategicpatience', 'stir', 'stereo', 'station', 'starter', 'stands', 'stabbing', 'stab', 'sprinter', 'spend', 'speaking', 'speaker', 'sparks', 'spain', 'sp', 'southern', 'southeast', 'southampton', 'sources', 'source', 'soup', 'sophie', 'someones', 'smithsonian', 'smile', 'slow', 'slide', 'slanglucci', 'skinny', 'sixth', 'six', 'sisters', 'simulate', 'silence', 'shell', 'sets', 'senate', 'selfie', 'section', 'sean', 'scott', 'score', 'scientists', 'schools', 'scheme', 'schedule', 'sat', 'sandiego', 'sanctions', 'safer', 'sa', 'rs', 'rose', 'rolling', 'rohingya', 'rockyfire', 'rockin', 'rocket', 'robots', 'robert', 'rob', 'rises', 'ring', 'rights', 'ridiculous', 'rider', 'ride', 'richard', 'reward', 'reveals', 'responsibility', 'republican', 'reportedly', 'replacing', 'remembered', 'regular', 'refugee', 'redlight', 'recommend', 'recognize', 'recently', 'recent', 'recall', 'reap', 'realize', 'reality', 'reaching', 'reach', 'rapidly', 'raining', 'rage', 'radar', 'racist', 'race', 'pulls', 'pulled', 'ps', 'providence', 'protest', 'prompts', 'promises', 'promise', 'progress', 'produced', 'probe', 'pro', 'prison', 'prior', 'print', 'prince', 'primary', 'previously', 'prepared', 'pregnant', 'predicted', 'predict', 'precipitation', 'prabhu', 'poverty', 'posted', 'positive', 'pls', 'players', 'plate', 'planted', 'plant', 'plaguing', 'pitch', 'pipeline', 'photoshop', 'phones', 'perhaps', 'pepper', 'pbban', 'path', 'passenger', 'parking', 'palms', 'palestinians', 'palestine', 'painting', 'packs', 'pacific', 'pace', 'overnight', 'outdoor', 'otrametlife', 'oppressions', 'opposite', 'opinion', 'opened', 'oops', 'omfg', 'olympic', 'okanagan', 'often', 'offers', 'occasion', 'nw', 'nurse', 'nu', 'np', 'notice', 'nobody', 'nigeria', 'nicki', 'newyork', 'newlyweds', 'network', 'nema', 'negative', 'nb', 'nasa', 'mystery', 'muslim', 'muscle', 'multiple', 'mtvhottest', 'mourning', 'mountains', 'motorcycle', 'mother', 'morgan', 'monsoon', 'minds', 'min', 'michigan', 'michael5sos', 'miami', 'mexico', 'messenger', 'mens', 'memes', 'measurement', 'meant', 'meals', 'materials', 'maryland', 'marked', 'mark', 'mansion', 'mama', 'mall', 'majority', 'magic', 'machine', 'lunch', 'lulgzimbestpicts', 'lowly', 'losses', 'losing', 'los', 'longest', 'lonely', 'logo', 'location', 'loan', 'load', 'lmfao', 'listed', 'lip', 'lightening', 'lez', 'leveled', 'letting', 'letters', 'lethal', 'legit', 'leaves', 'league', 'leaders', 'largest', 'kyle', 'known', 'kind', 'kerricktrial', 'kept', 'kca', 'karymsky', 'jst', 'joy', 'jordan', 'jeans', 'japÌn', 'i\\x89Û', 'iranian', 'invalid', 'internal', 'intern', 'interested', 'interest', 'insurers', 'instagram', 'innocent', 'include', 'impossible', 'images', 'ig', 'idp', 'idk', 'idfire', 'hurt', 'huh', 'hughes', 'huffman', 'httptcozujwuiomb3', 'httptcowvj39a3bgm', 'httptcoviwxy1xdyk', 'httptcothoyhrhkfj', 'httptcolvlh3w3awo', 'httpstcomoll5vd8yd', 'hosting', 'host', 'horses', 'horrific', 'hop', 'honors', 'holy', 'holland', 'holiday', 'hole', 'hitting', 'hills', 'highest', 'hieroglyphics', 'here\\x89Ûªs', 'heights', 'heaven', 'headphones', 'heading', 'haunting', 'hated', 'harry', 'harbor', 'haram', 'gusty', 'gusts', 'groups', 'grief\\x89Ûª', 'grenade', 'grazed', 'grateful', 'grab', 'gift', 'garden', 'gained', 'g', 'fwy', 'further', 'fund', 'fuel', 'fucked', 'frontline', 'forward', 'forgotten', 'forgot', 'forbes', 'follows', 'fly', 'floyds', 'flooded', 'flew', 'flags', 'fits', 'fingers', 'finds', 'fights', 'fifth', 'fettilootch', 'feeding', 'fbi', 'father', 'fate', 'faster', 'farrakhan', 'fantastic', 'famous', 'falcon', 'faith', 'factory', 'explains', 'expert', 'exist', 'ex', 'eventually', 'events', 'europe', 'estate', 'equal', 'epilepsy', 'entered', 'enter', 'enjoying', 'english', 'emotions', 'elevated', 'elem', 'electric', 'election', 'efforts', 'edinburgh', 'eden', 'ebike', 'eating', 'easily', 'earrings', 'earners', 'dtn', 'dry', 'drugs', 'drove', 'driven', 'dress', 'dragon', 'doubt', 'dk', 'diss', 'dinner', 'diamond', 'detectado', 'desire', 'designs', 'design', 'description', 'derails', 'deputies', 'dependency', 'democracy', 'delays', 'degree', 'defend', 'decisions', 'decision', 'decide', 'dear', 'daughters', 'daughter', 'dam', 'dallas', 'cyclist', 'current', 'cupcake', 'cup', 'crowd', 'crater', 'crashes', 'cr', 'cousin', 'couples', 'counselor', 'cotton', 'cos', 'cord', 'copycat', 'constantly', 'concerns', 'concern', 'compliant', 'communities', 'commercial', 'comments', 'collapses', 'cofounder', 'cock', 'clouds', 'cloud', 'clinton', 'client', 'cleveland', 'clev', 'clearly', 'cleanup', 'clash', 'claim', 'chill', 'childhood', 'chicagoarea', 'chelsea', 'charts', 'charlie', 'charles', 'channel', 'challenge', 'certainly', 'cell', 'cdc', 'caution', 'cats', 'catch', 'cast', 'carryi', 'captured', 'capture', 'captain', 'capacity', 'cancel', 'campus', 'californias', 'calamity', 'bulletin', 'bug', 'buckle', 'brutally', 'brothers', 'brings', 'braking', 'boxer', 'bound', 'boot', 'bluetooth', 'bluejays', 'blows', 'blocking', 'bless', 'bitches', 'bid', 'bff', 'beyonce', 'benefits', 'bell', 'begins', 'bees', 'beer', 'bedroom', 'beats', 'bears', 'battling', 'battery', 'bathroom', 'base', 'bands', 'bake', 'averted', 'average', 'av', 'auth', 'aussie', 'auctions', 'attacking', 'atlanta', 'assistance', 'asian', 'asia', 'arabian', 'apple', 'anybody', 'answers', 'ankle', 'angeles', 'analysis', 'america\\x89Ûªs', 'americas', 'allowed', 'alleged', 'alert', 'alcohol', 'alberta', 'alaska', 'agreed', 'admits', 'active', 'activates', 'accuses', 'ability', 'abbswinston', '852015', '80s', '800', '6aug', '5pm', '5000', '48', '3rd', '39', '370', '360wisenews', '34', '2pm', '2pcs', '29072015', '23', '21', '2030', '2009', '18w', '13000', '125', '1200', '10th', '02', 'å¨', '\\x89Û÷extremely', '\\x89ÛÏwe', '\\x89ÛÏrichmond', '\\x89ÛÏhatchet', 'zayn', 'yugvani', 'yorker', 'yep', 'yemen', 'yea', 'yahoonews', 'yahoo', 'xxx', 'wwi', 'wrapup', 'worldnetdaily', 'wom', 'wnd', 'witter', 'winter', 'willing', 'widespread', 'wicked', 'wi', 'wht', 'whales', 'weight', 'wee', 'weathernetwork', 'wear', 'weak', 'waterways', 'waterresistant', 'washed', 'warships', 'warned', 'warne', 'warn', 'warcraft', 'wannabe', 'walls', 'walker', 'voter', 'volga', 'vladimir', 'visited', 'virus', 'virgil', 'vip', 'villages', 'villagers', 'vid', 'vet', 'vest', 'vermont', 'verdict', 'venezuela', 'vegetarian', 'vegas', 'variety', 'vampiro', 'vacation', 'usnwsgov', 'urgent', 'unsafe', 'unknown', 'uniform', 'underway', 'undercover', 'unawares', 'unavoidable', 'ud', 'uber', 'tyre', 'ty', 'twins', 'turning', 'tunnel', 'tune', 'ts', 'trubgme', 'trs', 'triple', 'tries', 'trials', 'transporting', 'transformation', 'transfer', 'trafford', 'tr', 'towards', 'torch', 'tooth', 'tool', 'tonights', 'tomorrows', 'tmp', 'titan', 'tinyjecht', 'timeline', 'tilnow', 'til', 'tied', 'tie', 'ticket', 'thru', 'threats', 'third', 'thinks', 'thin', 'thick', 'the\\x89Û', 'theyd', 'therapy', 'that\\x89Ûªs', 'tflbusalerts', 'tf', 'tested', 'terrifying', 'terms', 'temper', 'tells', 'telegraph', 'teaching', 'teachers', 'tdp', 'tb', 'taxiways', 'targets', 'talks', 'talent', 'taco', 'tab\\x89Û', 'systems', 'sympathy', 'swing', 'swells', 'sweden', 'suv', 'survivor', 'surge', 'surface', 'surf', 'surely', 'surah', 'supervisor', 'superhero', 'suffering', 'suffered', 'stylish', 'stuck', 'strikes', 'stressed', 'stomach', 'stewart', 'stepped', 'stephen', 'step', 'steam', 'stays', 'startup', 'starring', 'standing', 'stamp', 'spinningbot', 'spinning', 'speed', 'specimens', 'specific', 'specif', 'spears', 'souls', 'songs', 'soldier', 'socialism', 'smoky', 'sm', 'slowly', 'slower', 'slip', 'slate', 'skirt', 'skills', 'sized', 'sittwe', 'sitting\\x89Û', 'sites', 'sink', 'silvergray', 'silent', 'significant', 'sight', 'showcase', 'shoulders', 'shook', 'shocked', 'shock', 'shitty', 'shirt', 'shipwreck', 'shelby', 'shedding', 'shared', 'shanghai', 'sexy', 'settlement', 'sends', 'sending', 'seemed', 'seasons', 'screenshots', 'scotland', 'scoopit', 'scifi', 'schiphol', 'scars', 'sb', 'savebees', 'sassy', 'sand', 'sampling', 'salvation', 'salmon', 'safely', 'saddlebrooke', 'rwy', 'rworldnews', 'runner', 'ruined', 'royal', 'rome', 'roller', 'role', 'robotrainstorm', 'robinson', 'roberts', 'roanoke', 'ripped', 'rid', 'rickperry', 'rexyy', 'revolution', 'reviews', 'revenues', 'returns', 'returned', 'retail', 'resulted', 'restaurant', 'responded', 'republic', 'repeat', 'reopening', 'reminder', 'relax', 'regret', 'registered', 'regarding', 'refused', 'reflect', 'referring', 'recycling', 'recover', 'recipes', 'recalls', 'realtime', 'realmandyrain', 'realdonaldtrump', 'raynbowaffair', 'range', 'ramag', 'rainfall', 'raid', 'radioactive', 'quotes', 'queens', 'purchase', 'pun', 'pull', 'ptsdchat', 'ptsd', 'psychological', 'psychiatric', 'provide', 'protests', 'propertycasualty', 'prompted', 'profit', 'proceeds', 'procedures', 'private', 'preview', 'present', 'premium', 'premature', 'prefer', 'prayers', 'posting', 'possibly', 'portion', 'porn', 'popular', 'pond', 'poland', 'pockets', 'plays', 'planning', 'pizza', 'pisgah', 'pillow', 'pieces', 'photographer', 'pets', 'period', 'peoples', 'penalties', 'peaceful', 'pattern', 'patched', 'patch', 'passed', 'panel', 'pam', 'palin', 'pack', 'outflow', 'os', 'ornament', 'origin', 'orange', 'oral', 'opus', 'options', 'ops', 'opposition', 'oper', 'openly', 'ooh', 'ontario', 'omars', 'olive', 'older', 'olap', 'ohio', 'offensiveåÊcontent', 'offensive\\x89Ûª', 'odeon', 'odds', 'odd', 'occurs', 'occupants', 'oc', 'oak', 'nyt', 'numbered', 'nuggets', 'nsfw', 'ns', 'nri', 'noticed', 'note', 'nose', 'noncompliant', 'non', 'noise', 'nm', 'nike', 'niece', 'nickcannon', 'nfl', 'newsintweets', 'neither', 'needle', 'necessary', 'nbcnews', 'navbl', 'nations', 'nasty', 'nap', 'musician', 'multiplayer', 'mullah', 'msf', 'movement', 'mountaineering', 'moth', 'moral', 'modiministry', 'modi', 'models', 'model', 'mnpdnashville', 'mkx', 'mitt\\x89Û\\x9d', 'mitigation', 'mistake', 'mirage', 'mins', 'minority', 'minor', 'minaj', 'milkshake', 'milk', 'militant', 'mike', 'mfs', 'mf', 'metrofmtalk', 'metrics', 'method', 'mercy', 'mercury', 'mentioned', 'mentally', 'mental', 'memphis', 'member', 'melt', 'mega', 'meat', 'mcilroy', 'mcdonalds', 'mattson', 'matters', 'mate', 'matches', 'marvel', 'marker', 'marine', 'marijuana', 'manmade', 'maj', 'magnum', 'ma', 'm194', 'luis', 'lrt', 'lover', 'loses', 'lonewolffur', 'log', 'locker', 'locke', 'lock', 'local\\x89Û', 'localarsonist', 'listenlive', 'listenbuy', 'limit', 'lighting', 'lgbt', 'lessons', 'lesbian', 'leo', 'len', 'leisure', 'lego', 'legion', 'legendary', 'leg', 'leadership', 'lay', 'launch', 'laugh', 'lately', 'las', 'languages', 'langley', 'landfall', 'laid', 'l', 'kuwait', 'kurtschlichter', 'kurdish', 'korea', 'komen', 'knoxville', 'kingdom', 'kiernan', 'kicked', 'key', 'kenya', 'karachi', 'kalle', 'just\\x89Û', 'jupiters', 'julie', 'journalist', 'jon', 'joke', 'joining', 'johnson', 'johannesburg', 'joel', 'jimmyfallon', 'jesus', 'jazz', 'jays', 'jay', 'jax', 'japans', 'jan', 'james', 'jail', 'j', 'i\\x89Ûªve', 'itunesmusic', 'irish', 'ireland', 'involved', 'integrity', 'instant', 'installation', 'inst', 'inning', 'ink', 'initial', 'inevitable', 'indonesia', 'individuals', 'individual', 'independent', 'imported', 'illinois', 'ik', 'idps', 'idiot', 'icymi', 'ibooklove', 'ianhellfire', 'ian', 'hybrid', 'hurry', 'hunk', 'hunger', 'hundred', 'humidity', 'humaza', 'httptcoo91f3cyy0r', 'httptconnmqlz91o9', 'httptcocybksxhf7d', 'httptcocedcugeuws', 'httptcobbdpnj8xsx', 'httptco9nwajli9cr', 'httpstcorqwuoy1fm4', 'httpstcodehmym5lpk', 'however', 'houston', 'horrors', 'honey', 'historic', 'hillary', 'hike', 'highly', 'higher', 'helps', 'headset', 'hating', 'hatchet', 'hatcap', 'hashtag', 'harrybecareful', 'harper', 'happiness', 'hang', 'hampshire', 'hamilton', 'hamas', 'haiyan', 'gunshot', 'gunsense', 'guilty', 'guided', 'guard', 'growth', 'grown', 'grove', 'groom', 'grew', 'greg', 'greeces', 'grand', 'grabbers', 'gps', 'gopdebate', 'gon', 'goals', 'glorious', 'gives', 'gerenciatodos', 'georgia', 'genuine', 'generally', 'gem', 'gateau', 'gamergate', 'gameplay', 'fyi', 'funds', 'fuels', 'fuckin', 'frozen', 'frontpage', 'freezing', 'frank', 'fraction', 'foul', 'formed', 'forgiven', 'ford', 'footage', 'foot', 'flying', 'flower', 'floated', 'flaming', 'fkn', 'fixed', 'fitness', 'fit', 'fishing', 'finishing', 'finish', 'finger', 'filled', 'ferguson', 'feminist', 'feelings', 'fed', 'features', 'fd', 'fb', 'favourite', 'fashionable', 'faroeislands', 'farm', 'fam', 'fallen', 'fair', 'facts', 'factors', 'extends', 'exploit', 'experiencing', 'experienced', 'expensive', 'expecting', 'existence', 'excellent', 'example', 'evidence', 'euro', 'eu', 'esp', 'er', 'eq', 'environmental', 'environment', 'entering', 'enrt', 'enjoyed', 'engine', 'ending', 'encounter', 'en', 'embroidered', 'email', 'elite', 'eight', 'ego', 'eggs', 'egged', 'ee', 'editor', 'ed', 'eb', 'eaten', 'eastern', 'earthquakes', 'ear', 'dundee', 'dudes', 'dublin', 'dual', 'drives', 'drinks', 'dreams', 'drag', 'dq', 'dorrets', 'dorret', 'donate', 'donald', 'domestic', 'doc', 'dj', 'disgusting', 'discovery', 'disasters', 'disappoints', 'disappearance', 'dis', 'dirt', 'diamondkesawn', 'devalue', 'detained', 'destroying', 'despite', 'designed', 'describing', 'describes', 'dept', 'dental', 'denmark', 'dem', 'deliver', 'delay', 'definitely', 'declined', 'december', 'decades', 'dating', 'dat', 'darkness', 'daesh', 'cutting', 'curb', 'cunts', 'cum', 'cue', 'cuban', 'cta', 'csx', 'cryptic', 'crosses', 'croatian', 'cries', 'cricket', 'crew', 'crashing', 'crackdown', 'counter', 'count', 'correction', 'corners', 'cooler', 'control\\x89Û\\x9d', 'continued', 'context', 'contemplating', 'contained', 'construction', 'consequence', 'conquest', 'connector', 'condition', 'concept', 'commute', 'communication', 'commit', 'comedy', 'combat', 'columbia', 'color', 'collision1141', 'college', 'cobra', 'coastal', 'coach', 'cnbc', 'clothes', 'closures', 'closing', 'closes', 'climb', 'clearedincident', 'claytonbryant', 'cladding', 'citizens', 'circuit', 'circle', 'cigarette', 'chunks', 'chosen', 'choking', 'choices', 'chocolate', 'chicken', 'chewing', 'chevy', 'chest', 'chernobyl', 'checked', 'chases', 'charlotte', 'charges', 'charger', 'chaos', 'changed', 'chain', 'centipede', 'cecilthelion', 'cd', 'cbcca', 'cave', 'catches', 'casual', 'casper', 'carried', 'cargo', 'capsized', 'capital', 'candy', 'camera', 'cam', 'calum5sos', 'cakes', 'caitlin', 'cabin', 'bypass', 'buying', 'butt', 'burnt', 'buried', 'bunch', 'bully', 'buddys', 'bu', 'btw', 'bts', 'bs', 'brush', 'briefing', 'brian', 'breathe', 'breaks', 'breakingnews', 'brave', 'brand', 'brady', 'bp', 'bowling', 'bounty', 'bottom', 'bother', 'born', 'bored', 'bore', 'border', 'boom', 'bookboost', 'bones', 'boko', 'boeing', 'bluedio', 'blowout', 'blowmandyup', 'blow', 'blasts', 'blanket', 'blame', 'blake', 'blah', 'birth', 'birds', 'bio', 'billings', 'bill', 'biker', 'bigamist', 'beware', 'betrayed', 'besides', 'bend', 'belongs', 'belonged', 'beginners', 'beforeitsnews', 'bee', 'becoming', 'beard', 'bbcnews', 'bb4sp', 'battleship', 'batters', 'bath', 'basement', 'bars', 'barn', 'barely', 'bare', 'barackobama', 'bangladesh', 'band', 'background', 'a\\x89Û', 'aware', 'aw', 'avenue', 'aussies', 'at\\x89Û', 'attitude', 'asshole', 'assembly', 'arwx', 'arts', 'artistsunited', 'arsenal', 'arrest', 'arm', 'appreciate', 'applications', 'apch', 'apartment', 'apart', 'aoms', 'anxious', 'anxiety', 'announcement', 'anna', 'angels', 'amirite', 'although', 'alternatives', 'alright', 'alex', 'alarms', 'ahh', 'affects', 'adding', 'acute', 'actual', 'acting', 'acid', 'accused', 'accionempresa', 'abortion', 'abomination', 'abe', 'abandon', '95', '87', '86', '80', '78', '6th', '69', '56', '55', '45', '43rd', '41', '375', '361', '300000', '2us', '27', '2005', '200', '1620', '150foot', '150401', '150', '103', '0day', '09', '010401', '0104', 'åÈ', 'åÇ', 'å', '\\x89Û÷plot', '\\x89Û÷it\\x89Ûªs', '\\x89ÛÏyou', '\\x89ÛÏstretcher', '\\x89ÛÏhannaph\\x89Û\\x9d', '\\x89Û¢åÊdemolition', 'zippednews', 'zionism', 'zimbabwe', 'zaynmalik', 'zabadani', 'yycstorm', 'ypres', 'you\\x89Ûªre', 'you\\x89Ûªll', 'yourselves', 'yougov', 'yesterdays', 'yellow', 'yelling', 'yazidi', 'yankees', 'xv', 'wwwcbplawyers', 'wwe', 'ww2', 'wud', 'wrist', 'wowo', 'wouldn\\x89Ûªt', 'worries', 'worn', 'worldwide', 'works', 'workplace', 'workout', 'worker', 'woods', 'woodlawn', 'wolves', 'wolf', 'wod', 'wocowae', 'wmv', 'wmur9', 'witness', 'wipp', 'wine', 'windy', 'wilshere', 'william', 'wide', 'whose', 'whoops', 'whomever', 'wholesale', 'whoever', 'whoa', 'whitehouse', 'whitbourne', 'whistle', 'whipped', 'whenever', 'whatsapp', 'whale', 'wftv', 'wfp', 'weyreygidi', 'weston', 'wen', 'welfare', 'weighs', 'weekold', 'wednesdays', 'wedn', 'wedaug5th', 'wealthy', 'weallheartonedirection', 'weakening', 'wd', 'wce', 'wbioterrorismampuse', 'wb', 'wayne', 'wattpad', 'watertown', 'waters', 'washingtonpost', 'warrior', 'warming', 'warm', 'walter', 'walked', 'walerga', 'waking', 'waits', 'waist', 'voting', 'voted', 'voodoo', 'voluntary', 'volleyball', 'volcanoåÊinåÊrussia', 'volcanoes', 'vods', 'vital', 'visto', 'visits', 'vision', 'violations', 'vince', 'vietnamese', 'videoveranomtv', 'vida', 'victory', 'vicinity', 'vice', 'vibez', 'vi', 'veteran', 'versions', 'versethe', 'venice', 'vegetable', 've', 'vault', 'vashon', 'various', 'vanuatu', 'vantage', 'vancouver', 'utc5km', 'ut', 'ushed', 'user', 'us101', 'urs', 'upwards', 'uploading', 'upgrading', 'upgraded', 'unrest', 'unrelenting', 'unr', 'unprecedented', 'unnecessary', 'unity', 'unique', 'unions', 'uniforms', 'unhappiness', 'unfortunately', 'unfolded', 'uncle', 'ultimalucha', 'ukraine', 'typos', 'types', 'tyler', 'twovehicle', 'twos', 'twentynine', 'tweetlikeitsseptember11th2001', 'turkmen', 'tuning', 'tunein', 'tuesday', 'ttes', 'tt', 'tsunamiesh', 'tryouts', 'tryout', 'trusty', 'trunk', 'truelove', 'troy', 'troubling', 'troll', 'trinity', 'trillion', 'trick', 'trend', 'tremor', 'trek', 'treeporn', 'treatments', 'treating', 'treated', 'travis', 'traverse', 'travelling', 'traveling', 'transformed', 'trancy', 'tragic', 'trading', 'touched', 'totes', 'torture', 'tornados', 'torching', 'tor', 'topstories', 'tops', 'tomcatarts', 'tomatoes', 'tokyo', 'tlc', 'tix', 'titanic', 'tin', 'timkaine', 'timelapse', 'tigers', 'tides', 'tidal', 'thyroid', 'thy', 'thuggin', 'thu', 'thrown', 'throwingknifes', 'throwback', 'throughout', 'thrones', 'throat', 'threw', 'threaten', 'thisizbwright', 'thisiswhywecanthavenicethings', 'thirsty', 'thirst', 'thighs', 'thief', 'thetawniest', 'therapies', 'theological', 'themed', 'themagickidraps', 'thegame', 'thee', 'thankfully', 'thankful', 'tha', 'tgirl', 'tests', 'testing', 'testimony', 'teslas', 'term', 'tens', 'temptation', 'teeth', 'teens', 'teenagers', 'technologies', 'technique', 'technica', 'teases', 'tear', 'teamstream', 'teamhendrick', 'teacher', 'te', 'tch', 'tc', 'taylor', 'taught', 'tattoo', 'tastes', 'tasmanias', 'targeted', 'tape', 'tanzania', 'tanks', 'tan', 'talkin', 'tale', 'tak', 'tahoe', 's\\x89Û', 'syndrome', 'symphony', 'sx', 'sws', 'swept', 'sweet2young', 'sweater', 'sweat', 'swear', 'swarm', 'sustainable', 'suspense', 'suryaray', 'survey', 'suruc', 'surrounding', 'surrender', 'surgery', 'surfers', 'suppose', 'supports', 'supply', 'sunny', 'sunnis', 'sundays', 'summit', 'summertime', 'suit', 'suicidebombing', 'suh', 'suddenly', 'sudden', 'subway', 'stuns', 'stunned', 'studio', 'student', 'stu', 'sts', 'struggling', 'strongly', 'stronger', 'stroke', 'striking', 'strikesstrikes', 'strikers', 'striker', 'strict', 'stressful', 'strengthening', 'stout', 'stormchase', 'stores', 'stolen', 'stole', 'stockton', 'stocks', 'sticks', 'stick', 'steal', 'steak', 'stats', 'state\\x89Û', 'startups', 'starbucks', 'stans', 'standwithpp', 'standards', 'stagetwo', 'stacey', 'sr', 'squeeze', 'square', 'sputnik', 'spreading', 'spots', 'sportwatch', 'spontaneously', 'split', 'spilled', 'spending', 'spencers', 'spectacular', 'species', 'specialists', 'specialist', 'speak', 'soviet', 'southdowns', 'soudelors', 'sort', 'sorrows', 'sore', 'soooo', 'sooo', 'soo', 'sons', 'somewhere', 'solve', 'solo', 'solitude', 'solid', 'solicitor', 'sold', 'software', 'softenza', 'soft', 'socialmedia', 'soccer', 'soaking', 'soak', 'snuff', 'snowden', 'snowball', 'sniping', 'sneak', 'snd', 'snapchat', 'snacks', 'smooth', 'smiling', 'smells', 'smell', 'smash', 'slips', 'slipped', 'slides', 'slicker', 'slayer', 'slammed', 'skys', 'skyline', 'skipping', 'skill', 'sketch', 'skanndtyagi', 'sixpenceee', 'sixmeter', 'sis', 'singles', 'sin', 'simultaneous', 'simply', 'simon', 'signup', 'signing', 'sigalert', 'siding', 'sides', 'si', 'shutdown', 'showing', 'showed', 'shouting', 'shout', 'shouldve', 'shore', 'shooter', 'shoot', 'shoe', 'shizune', 'shira', 'sheriff', 'shelli', 'sheet', 'sheeran', 'sheer', 'shed', 'shaw', 'sharing', 'sharethis', 'shantae', 'shania', 'sham', 'shaking', 'shakes', 'shaker', 'sf', 'sewer', 'settle', 'setlist', 'servers', 'server', 'serve', 'servants', 'sequel', 'sept', 'sep', 'selling', 'sell', 'selfies', 'selection', 'seem', 'seduction', 'seal', 'screw', 'screamqueens', 'scream', 'scottwalker', 'scorpions', 'schwarber', 'schedule\\x89Û', 'scares', 'scare', 'scale', 'saturn', 'satan', 'sarcasm', 'sandy', 'sanctioned', 'samesex', 'samanthaturne19', 'salvis', 'salvador', 'sake', 'sailing', 'sail', 'sahib', 'sacramento', 'sa15t', 's2g', 'ryan', 'rutherford', 'rush', 'ruins', 'ruining', 'ruebs', 'rudd', 'rtrrt', 'rtamerica', 'rss', 'rpics', 'royals', 'royalcarribean', 'rover', 'routecomplex', 'rotten', 'rotator', 'rossum', 'roses', 'rory', 'ronnie', 'ronaldo', 'rolls', 'rohnertparkdps', 'roh3', 'rogue', 'roger', 'rockbottomradfm', 'riyadh', 'riversiskiyou', 'risks', 'rio', 'rihanna', 'rightwaystan', 'rightways', 'rifles', 'riding', 'riders', 'rick', 'rich', 'rey', 'revelation', 'rev', 'retweet', 'retro', 'resulting', 'restricted', 'restive', 'restaurants', 'responder', 'respected', 'resources', 'reshapes', 'required', 'request', 'repay', 'repairs', 'rep', 'reopens', 'reopen', 'renison', 'removing', 'removed', 'removal', 'remorse', 'remix', 'religion', 'relentless', 'relationship', 'rejects', 'rejected', 'reject', 'reinstate', 'reids', 'reid', 'regional', 'regardless', 'regard', 'refuse', 'redeemeth', 'recreates', 'recording', 'recommended', 'receive', 'recap', 'realised', 'reads', 'reacted', 'react', 'reaches', 'rdhorndale', 'rcmp', 'rb', 'raÌ¼l', 'razing', 'raynor', 'rayner', 'raw', 'raung', 'rating', 'rates', 'rated', 'rat', 'rapping', 'rappers', 'rapper', 'rapidcity', 'rap', 'random', 'rammed', 'raised', 'rainier', 'rained', 'railways', 'railway', 'rails', 'raiders', 'rages', 'racing', 'quoted', 'questioning', 'que', 'quartz', 'quarters', 'q', 'p\\x89Û', 'putins', 'pushing', 'push', 'purchased', 'puppy', 'puppies', 'punjab', 'punishment', 'punished', 'puncture', 'pulwama', 'pulling', 'publicizing', 'ptbo', 'pt', 'psalms', 'ps4', 'proud', 'protesting', 'protection', 'protecting', 'protectdenaliwolves', 'prosyn', 'proposed', 'proper', 'proof', 'prone', 'promote', 'projects', 'programme', 'professional', 'product', 'prod', 'privacy', 'priority', 'priorities', 'primarily', 'priceless', 'previous', 'prevention', 'pretend', 'pressed', 'preservation', 'presence', 'prepper', 'preparing', 'preorder', 'premonitions', 'premiere', 'premier', 'prediction', 'predator', 'precious', 'pre', 'potentially', 'posters', 'postbattle', 'possibility', 'poss', 'portugal', 'popularmmos', 'pops', 'poplar', 'pope', 'poorly', 'pomo', 'politifiact', 'policerun', 'pointing', 'poem', 'pocket', 'po', 'pmharper', 'pmarca', 'pluto', 'plummeted', 'pllolz', 'plenty', 'pledge', 'pleasure', 'platform', 'plants', 'planes', 'placing', 'placed', 'pjnet', 'pits', 'pitchers', 'pissed', 'pinpoint', 'piner', 'pillows', 'pill', 'pictures', 'pickup', 'pickerel', 'physicians', 'photoset', 'philadelphia', 'phil', 'phew', 'phase', 'pharaoh', 'phantom', 'peterjukes', 'pertains', 'personally', 'perquisite', 'permits', 'permanently', 'permanent', 'performing', 'performance', 'per', 'pennington', 'penn', 'pendleton', 'peeps', 'pedestrians', 'pedestrian', 'peak', 'peacefully', 'pdx911', 'pcps', 'pays', 'paved', 'pattonoswalt', 'patriot', 'pathogen', 'pasta', 'password', 'partnerships', 'particularly', 'paris', 'paramedic', 'parade', 'pants', 'pan', 'pakpattan', 'pakistannews', 'pair', 'paint', 'paid', 'pa', 'ovofest', 'ovo', 'overwork', 'overlooking', 'overload', 'ouvindo', 'outfit', 'outbid', 'outage', 'ourselves', 'otherwise', 'oth', 'oso', 'osborn', 'originalfunko', 'organizations', 'organic', 'org', 'oregon', 'ordered', 'opp', 'operations', 'opera', 'oped', 'oooooohhhh', 'oooh', 'oo', 'onlinecommunities', 'oneself', 'omega', 'oks', 'okinawa', 'ohh', 'offs', 'offramp', 'offr', 'offices', 'offers2go', 'obviously', 'obsessed', 'objects', 'obispo', 'nylon', 'nwo', 'nv', 'nurses', 'nuff', 'nude', 'ntsb', 'nowhere', 'november', 'novel', 'notifications', 'noted', 'nosurrender', 'nostrils', 'nostalgia', 'northwest', 'northland', 'northeast', 'norman', 'nor', 'nope', 'nonlife', 'noaa', 'njturnpike', 'njenga', 'nj', 'nixon', 'nikeplus', 'nights', 'nightmare', 'nickcocofree', 'nick', 'niall', 'nhs', 'nh', 'new\\x89Û', 'newzsacramento', 'newswatch', 'newsarama', 'newbie', 'newberg', 'neverending', 'netanyahu', 'nestleindia', 'nepal', 'neighborhood', 'neck', 'nda', 'nbc', 'nba', 'nazi', 'native', 'nasasolarsystem', 'narendramodi', 'nankana', 'nan', 'names', 'najib', 'nail', 'm\\x89Û', 'myfitnesspal', 'mv', 'muzzamil', 'mutual', 'mutant', 'musik', 'musicians', 'murders', 'murderers', 'mum', 'multidimensi', 'multi', 'mukilteo', 'mud', 'msnbc', 'moyo', 'mouth', 'mourns', 'moulding', 'motion', 'moscow', 'mooresville', 'montgomery', 'montetjwitter11', 'monster', 'monkeys', 'monkey', 'monitoring', 'monitor', 'moms', 'mohammed', 'moderate', 'mock', 'mobile', 'mmmmmm', 'ml', 'miyagi', 'mistreated', 'mississauga', 'missionhills', 'missiles', 'misses', 'mishacollins', 'misery', 'ministers', 'minions', 'minimehh', 'minhazmerchant', 'milwaukee', 'migrants\\x89Û', 'midwest', 'midsouth', 'midget', 'mid', 'microsofts', 'microsoft', 'microphone', 'micom', 'michelebachman', 'mic', 'mi17', 'mgm', 'metlife', 'meteoearth', 'met', 'messi', 'messed', 'messages', 'mercados', 'meme', 'meinlcymbals', 'megynkelly', 'meets', 'medium', 'medieval', 'medicine', 'medals', 'mechanical', 'measles', 'meaning', 'mcmahon', 'mchenry', 'mc', 'matthew', 'material', 'matching', 'master', 'massmurderer', 'martinmj22', 'marshall', 'married', 'marquei', 'marlon', 'marketing', 'marines', 'mariah', 'march', 'maps', 'manufactured', 'mania', 'mandatory', 'mamata', 'malik', 'maketh', 'maker', 'maintain', 'magginoodle', 'mafia', 'madison', 'lyrics', 'lying', 'luka', 'luchaunderground', 'lubbock', 'lovedup', 'lous', 'lotz', 'loseit', 'losdelsonido', 'loretta', 'looses', 'looping', 'longterm', 'longs', 'lone', 'londonfire', 'lolol', 'lollapalooza', 'logistics', 'logic', 'locked', 'located', 'loans', 'loads', 'loading', 'loaded', 'lmfaoooo', 'llf', 'lizards', 'livingsafely', 'liveonk2', 'lived', 'lith', 'lit', 'links', 'lines', 'likes', 'lightningcaused', 'lifting', 'lifetime', 'lifethreatening', 'lifestyle', 'liberal', 'liable', 'liability', 'lglorg', 'letter', 'letsfootball', 'lesson', 'lemon', 'legs', 'legislation', 'legio', 'leeds', 'learned', 'leaks', 'leaked', 'leak', 'layout', 'lawsuit', 'lawrence', 'lavenderpoetrycafe', 'lauren', 'laundry', 'latimes', 'latestnews', 'lasting', 'laois', 'lansdowne', 'language', 'lanes', 'landscape', 'lands', 'lance', 'lamha', 'lakes', 'labs', 'label', 'kurd', 'ks94', 'ks', 'kraft', 'kowing', 'koin6news', 'knowing', 'knob', 'knife', 'knees', 'km', 'kittens', 'kith', 'kisses', 'kissed', 'kiss', 'kings', 'kindness', 'kindermorgan', 'killings', 'killers', 'kiev', 'kidding', 'kerry', 'keratin', 'kendall', 'ke', 'katunews', 'katrina', 'katherines', 'kashmir', 'karma', 'kanye', 'kaduna', 'justmarried', 'justifying', 'justified', 'jurors', 'junk', 'june', 'jumped', 'juliedicaro', 'juice', 'jr', 'jp', 'journal', 'jose', 'jonvoyage', 'jones94kyle', 'jones', 'jonathanferrell', 'joint', 'johns', 'johnny', 'jkl', 'jim', 'jhaustin', 'jewelry', 'jerry', 'jenner', 'jeff', 'jealous', 'jdabe80', 'japton', 'jams', 'jamesmelville', 'jamaicaplain', 'jamaicaobserver', 'jailed', 'jacque', 'jacket', 'ja', 'i\\x89Ûªd', 'ivanberroa', 'itsjustinstuart', 'itll', 'items', 'item', 'isil', 'irony', 'ironic', 'iredell', 'iranians', 'ipod', 'ipad', 'in\\x89Û', 'investing', 'investigation', 'investigate', 'invest', 'invasion', 'interviews', 'interstate', 'interlaken', 'intelligence', 'intact', 'insubcontinent', 'inspiring', 'inspections', 'insomnia', 'insight', 'insas', 'innovation', 'injures', 'infosec', 'infinity', 'inferno', 'infectious', 'inevitably', 'inec', 'indoors', 'indie', 'indiannews', 'indi', 'incredibly', 'increase', 'incase', 'inc', 'impulse', 'improve', 'impressive', 'impressed', 'important', 'imperfect', 'impending', 'impacted', 'immediately', 'imdb', 'imagined', 'ima', 'illustration', 'ihhen', 'ignored', 'ignore', 'idol', 'idis', 'identitytheft', 'idc', 'iconic', 'icelandreview', 'i95', 'i580', 'i405', 'i10', 'h\\x89Û', 'hysteria', 'hypocrisy', 'hyderabad', 'hurricanes', 'hurricanedolce', 'humble', 'humanconsumption', 'hug', 'httpt\\x89Û', 'httptcozevakjapcz', 'httptcozdtoyd8ebj', 'httptcoyduixefipe', 'httptcoydetwgribk', 'httptcoxssgedsbh4', 'httptcoxpfmr368uf', 'httptcovz1irh0nmm', 'httptcovxvfaeey0q', 'httptcovam5podgyw', 'httptcov3azwoamzk', 'httptcouoozxaus26', 'httptcoskqpwsnoin', 'httptcosdgoutwntb', 'httptcosaf9mosksn', 'httptcoroi2nsmejj', 'httptcopyehwodwun', 'httptcopvmr38lnva', 'httptcopo19h8ycnd', 'httptcophixznv1yn', 'httptconmfsgkf1za', 'httptcomg5eajelul', 'httptcomfckpvzfv8', 'httptcom5kxlpkfa1', 'httptcom203ul6o7p', 'httptcolxtjc87kls', 'httptcolwwojxttiv', 'httptcojhpdssvhve', 'httptcoj5mkcbkcov', 'httptcoio7kuug1uq', 'httptcoiikssjgbdn', 'httptcoidmhswewqw', 'httptcoi27oa0hisp', 'httptcoeysvvza7qm', 'httptcoedyfo6e2pu', 'httptcodydfvz7amj', 'httptcoct9ejxolpu', 'httptcoc1h7jecfrv', 'httptcobtdjgwekqx', 'httptcoafmkcfn1tl', 'httptco7hanpcr5rk', 'httptco3tj8zjin21', 'httptco3sicroaanz', 'httptco0wratka2jl', 'httpstcowudlkq7ncx', 'httpstcolfkmtzaekk', 'httpstcoe8dl1lncvu', 'hr', 'housed', 'hottest', 'hostageamp2', 'hospitals', 'hoping', 'hood', 'honest', 'homs', 'homie', 'homeowners', 'holmgren', 'holed', 'holds', 'hoes', 'hoe', 'hockey', 'hmu', 'hmm', 'hiroshima70', 'hiphop', 'hinton', 'himalaya', 'hiding', 'hide', 'hidden', 'hew', 'heros', 'heroin', 'hermancranston', 'henry', 'helpline', 'hella', 'helicopters', 'heavenly', 'heartwarming', 'heartless', 'hearthstone', 'healthy', 'healthcare', 'healing', 'heal', 'headlines', 'hazmat', 'have\\x89Û', 'hatred', 'hatchetwielding', 'harwich', 'harmkid', 'hardy', 'hardline', 'hardcore', 'happily', 'handling', 'handle', 'hamburg', 'halt', 'hall', 'hahahaha', 'hahahah', 'hahaha', 'hahah', 'hah', 'hadnt', 'hackers', 'hack', 'habits', 'gym', 'gust', 'gunmen', 'gum', 'guest', 'guaranteed', 'gtii', 'growingupblack', 'growing', 'grounds', 'grenades', 'greenway', 'greenharvard', 'greek', 'greatbritishbakeoff', 'graveyard', 'gravel', 'grass', 'grandpa', 'grandeur', 'grande', 'gpm', 'governments', 'goulburn', 'gorgeous', 'goodbye', 'golem', 'goldstein', 'goku', 'gofundme', 'goat', 'gn', 'gmt', 'gmmbc', 'gloucester', 'globe', 'globalwarming', 'glink', 'glimpses', 'glenn', 'glasses', 'giveaway', 'gilbert23', 'gig', 'gif', 'ghostwriter', 'gg', 'geometric', 'genius', 'geneva', 'generalnews', 'gel', 'gear', 'gd', 'gays', 'gawx', 'gates', 'garfield', 'gardens', 'garbage', 'gander', 'gaining', 'gain', 'gadgets', 'ga', 'fury', 'function', 'fukushimatepco', 'fueling', 'fruits', 'frontlines', 'friggin', 'friendship', 'fresno', 'freshman', 'freaking', 'franklin', 'fran', 'fragile', 'foxnew\\x89Û', 'fouseytube', 'fourth', 'foster', 'for\\x89Û', 'fortune', 'fortunately', 'forth', 'forsure', 'forgive', 'forex', 'foreign', 'forbid', 'fool', 'foodscare', 'follower', 'folk', 'fold', 'fm', 'fluid', 'flow', 'florida\\x89Û', 'float', 'flip', 'fleeing', 'fled', 'flashbacks', 'fixing', 'fist', 'fire\\x89Û', 'firey', 'firetruck', 'firemen', 'firefighting', 'fir', 'finished', 'financing', 'fill', 'files', 'file', 'fifty', 'fifa16', 'fierce', 'fieg', 'fi', 'fevwarrior', 'fest', 'ferries', 'ferguson\\x89Ûªs', 'fergusons', 'fennovoima', 'fella', 'feinstein', 'federal', 'featured', 'feast', 'favs', 'favorites', 'favor', 'faulty', 'fathers', 'fatburning', 'fatally', 'fatalityus', 'fart', 'familia', 'fallacy', 'fairy', 'fairfax', 'fading', 'facing', 'facility', 'fabric', 'extinguished', 'exterminate', 'extensive', 'extension', 'extender', 'explores', 'explodes', 'explain', 'expand', 'exited', 'exhibition', 'exhausted', 'exercised', 'executing', 'excuse', 'exclusive', 'exciting', 'excited', 'exc', 'examiner', 'evolve', 'everytime', 'everybody', 'evansville', 'evanston', 'evacuating', 'ev', 'eurotunnel', 'eudrylantiqua', 'etisalat', 'et', 'esteemed', 'esh', 'escaping', 'escaped', 'ergo', 'equate', 'epicenter', 'en\\x89Û', 'envw98', 'entrepreneur', 'enroute', 'enormous', 'enhanced', 'engvaus', 'engage', 'enemies', 'endures', 'endorses', 'endangered', 'encouragement', 'encore', 'enabled', 'empty', 'empire', 'emperor', 'emotionally', 'emerges', 'emerg', 'elsa', 'elliott', 'elkhorn', 'elizabeth', 'element', 'electricity', 'eh', 'egypt', 'efak', 'ef5', 'edge', 'economies', 'eastward', 'ears', 'earnings', 'earned', 'earbuds', 'dystopian', 'dye', 'dwarves', 'dvc', 'durant', 'dunbar', 'dumb', 'dukes', 'ducks', 'duck', 'dt', 'ds', 'dryer', 'drum', 'dropping', 'droid', 'drivers', 'drifting', 'drew', 'dressed', 'dreaming', 'draw', 'drama', 'dozen', 'downstairs', 'downpours', 'downfall', 'douchebag', 'doublecups', 'dothraki', 'dorman', 'dopey', 'dope', 'doors', 'donå«t', 'dolphin', 'dollars', 'document', 'doctors', 'doctor', 'dock', 'dmpl', 'dm', 'dlh', 'divided', 'district', 'distinct', 'disruptive', 'displace', 'discussion', 'disappeared', 'disappear', 'directors', 'director', 'diplomacy', 'dijk\\x89Ûª', 'dignity', 'digits', 'digital', 'difficult', 'diesis', 'dickheads', 'dick', 'diarrhea', 'diaporama', 'diamorfiend', 'diablo', 'dey', 'devil', 'device', 'develop', 'detroit', 'destructive', 'destroyer', 'despair', 'desk', 'deserves', 'deserve', 'descriptions', 'derby', 'deputy', 'deny', 'denier', 'denial', 'denali', 'dems', 'demon', 'democrats', 'demi', 'delete', 'delayed', 'del', 'define', 'defects', 'def', 'deeds', 'decor', 'declared', 'deck\\x89Û\\x9d', 'deck', 'decent', 'debatequestionswewanttohear', 'dealbreaker', 'deadliest', 'daytoday', 'davidcameron', 'dates', 'darkest', 'dante', 'dannyonpc', 'danisnotonfire', 'daniels', 'dances', 'dambisa', 'damaging', 'cyclists', 'cycling', 'customs', 'customers', 'custom', 'custer', 'cus', 'curiosity', 'cuff', 'ctd', 'cs', 'crusty', 'crude', 'crowns', 'croat', 'critical', 'crippling', 'criminal', 'crimes', 'cried', 'crickets', 'credit', 'creating', 'creates', 'crane', 'cracks', 'cracking', 'crack', 'coyotes', 'coworker', 'cowboys', 'cow', 'coverage', 'cousins', 'courts', 'coursing', 'countynews', 'countrys', 'countless', 'couldve', 'costly', 'correspondent', 'correct', 'corp', 'corleonedaboss', 'corey', 'coral', 'copycats', 'coping', 'cope', 'cooking', 'conversations', 'conversation', 'contruction', 'controllers', 'controlled', 'contributing', 'contrast', 'contract', 'continually', 'contain', 'contact', 'constant', 'console', 'conservative', 'consent', 'connectorconnecto', 'condolence', 'condemnation', 'condemn', 'concrete', 'concluded', 'con', 'compound', 'complications', 'completed', 'complaints', 'compete', 'comparison', 'comp', 'como', 'committee', 'committed', 'commerce', 'command', 'colluded', 'collective', 'cole', 'colder', 'col', 'coincide', 'cod', 'cockpit', 'cocaine', 'coat', 'coaster', 'cnewslive', 'cmon', 'cm', 'clueless', 'closest', 'cliffs', 'clever', 'cleric', 'cleared', 'cld', 'claimed', 'cjoyner', 'civilization', 'cityofcalgary', 'cites', 'circus', 'chronicle', 'christie', 'christ', 'chris', 'choose', 'chills', 'chiefs', 'chick', 'cheyenne', 'chevrolet', 'chesttorso', 'cheryl', 'cherry', 'cherokee', 'chase', 'channels', 'changing', 'chances', 'chan', 'championship', 'challenges', 'chairs', 'chairman', 'chair', 'cfc', 'certified', 'certificate', 'centers', 'census', 'cement', 'celebrations', 'cbsbigbrother', 'cbs', 'cbc', 'cawx', 'catching', 'castle', 'casperrmg', 'cash', 'cart', 'carr', 'carpet', 'carlos', 'caribbean', 'careers', 'cape', 'canvas', 'cannon', 'candle', 'cancers', 'cancels', 'cancelled', 'canadas', 'camps', 'camping', 'cameo', 'calumet', 'cairo', 'cafire', 'cafe', 'cadfyi', 'cables', 'cab', 'c4news', 'b\\x89Û', 'bwp', 'butterfinger', 'busy', 'bundle', 'bumper', 'bump', 'bullseye', 'bullets', 'builds', 'buffer', 'bubble', 'brunette', 'bruise', 'brooke', 'bronx', 'britons', 'britney', 'brisk', 'brighton', 'brigade', 'brief', 'brick', 'brewing', 'breathing', 'bread', 'brazilian', 'bradleybrad47', 'boxing', 'bounds', 'boundary', 'boundaries', 'bottle', 'boise', 'bodys', 'bodybagging', 'bobcats', 'bmw', 'blutz10', 'blunt', 'blues', 'blueprint', 'bloomberg', 'blocks', 'blizzheroes', 'blizzarddraco', 'blames', 'blackpool', 'blacklivesmatter', 'bits', 'bites', 'bite', 'bitcoin', 'bistro', 'birmingham', 'biological', 'bills', 'billneelynbc', 'billboard', 'bicycles', 'bicycle', 'bicep', 'betz', 'bets', 'bethlehem', 'bestseller', 'bengal', 'belt', 'belly', 'believing', 'beliefs', 'belief', 'behalf', 'begging', 'beclearoncancer', 'beckarnley', 'beauty', 'bean', 'battles', 'batting', 'batteries', 'battered', 'basis', 'basically', 'bashes', 'baruch', 'barrier', 'banquet', 'bangin', 'banerjee', 'bancodeseries', 'baltimore', 'balls', 'balance', 'bakeofffriends', 'bail', 'bago', 'backyards', 'backyard', 'backup', 'backs', 'ay', 'awwww', 'awareness', 'awaits', 'avoided', 'aviation', 'avengers', 'available', 'autumn', 'autoinsurance', 'autistic', 'author', 'authentic', 'australia\\x89Ûªs', 'austin', 'aust', 'aunt', 'audience', 'auction', 'auckland', 'attraction', 'attic', 'attendance', 'attempting', 'attempt', 'atmospheric', 'atmosphere', 'atm', 'atlantic', 'atk', 'athlete', 'astrology', 'association', 'assisting', 'assistant', 'assholes', 'asleep', 'asks', 'askcharley', 'asics', 'ashley', 'ashayo', 'ash', 'asf', 'artist', 'artificial', 'arsonistmusic', 'ars', 'arriving', 'arnhem', 'armory', 'armed', 'arizona', 'ariaahrary', 'argument', 'area\\x89Û', 'architecture', 'architect', 'aquarium', 'apt', 'april', 'approves', 'approval', 'appropriation', 'approaching', 'appreciated', 'appointment', 'applies', 'applaud', 'apollobrown', 'apocalyptic', 'apartments', 'aogashima', 'anyways', 'any1', 'antonio', 'annoying', 'announces', 'announced', 'ani', 'anger', 'andrew', 'andor', 'ames', 'ambulances', 'alot', 'alois', 'ally', 'alloy', 'allies', 'allegiance', 'allegations', 'aliens', 'alien', 'alexbelloli', 'alerts', 'albany', 'alarmed', 'ak', 'agrees', 'aggressive', 'ages', 'agents', 'agency', 'ag', 'africans', 'afp', 'affiliation', 'affiliate', 'affecting', 'afc', 'advised', 'advice', 'advertised', 'adventures', 'advanced', 'admit', 'administration', 'admin', 'addition', 'adam', 'ad', 'actor', 'activities', 'acted', 'acquire', 'acoustic', 'achimota', 'accustomed', 'accounts', 'accept', 'acc', 'ac', 'abusing', 'abuseddesolateamplost', 'abused', 'abubaraa1', 'absolute', 'abomb', 'abia', 'abc7', 'abbott', 'aa', 'a5', 'a1', '9pm', '9newsgoldcoast', '9am', '96', '90th', '90blksamp8whts', '9000', '900', '8th', '8pin', '84', '83', '77', '75', '72w', '64', '61st', '60mph', '60000', '5th', '5sosfam', '5sos', '5s', '5c', '57', '548', '530', '53', '4th', '4playthursdays', '4km', '4500feet', '43', '429cj', '3m', '3inspired', '3942', '360', '36', '32', '2k15', '299', '29', '28', '23km', '235409', '233liveonline', '21a', '211023', '2082676773', '2016', '20150805', '2012', '2010', '2008', '2000', '1999', '1998', '1986', '1979', '1976', '1974', '1970', '1965', '1943', '1916', '18wheeler', '1880', '180', '175225', '1716', '166', '15th', '15km', '143', '140', '12news', '118', '1100', '10km', '109', '1061thetwister', '1038pm', '1030', '100000', '075', '0700', '070', '06jst', '0306', '015025', '005225', 'åÊi', 'åÊfedex', 'å¬only', 'å©daniel', 'å¤', 'å£9', 'å£6bn', 'å£27900end', 'å£150', 'å£100bn', 'Ìü', 'ÌÑ1', 'ÌÑ', 'Ì¢', '\\x89âÂ', '\\x89Û÷we', '\\x89Û÷vulnerable\\x89Ûª', '\\x89Û÷the', '\\x89Û÷second\\x89Ûª', '\\x89Û÷ransomware\\x89Ûª', '\\x89Û÷nuclear', '\\x89Û÷nother\\x89Û\\x9d', '\\x89Û÷muslim', '\\x89Û÷minimum', '\\x89Û÷let\\x89Ûªs', '\\x89Û÷leaves', '\\x89Û÷it', '\\x89Û÷institute', '\\x89Û÷ill', '\\x89Û÷hoax', '\\x89Û÷hijacker', '\\x89Û÷heat', '\\x89Û÷hazard\\x89Ûª', '\\x89Û÷good', '\\x89Û÷food', '\\x89Û÷first\\x89Ûª', '\\x89Û÷faceless\\x89Ûª', '\\x89Û÷faceless', '\\x89Û÷exceptional\\x89Ûª', '\\x89Û÷em', '\\x89Û÷devastated\\x89Ûª', '\\x89Û÷british', '\\x89Û÷bomb', '\\x89Û÷body', '\\x89Û÷badges', '\\x89Û÷avalanche\\x89Ûª', '\\x89Û÷amino', '\\x89Û÷alloosh', '\\x89ÛÓkody', '\\x89ÛÓher', '\\x89ÛÒåÊcnbc', '\\x89ÛÒthe', '\\x89ÛÏymcglaun', '\\x89ÛÏthehighfessions', '\\x89ÛÏthat\\x89Ûªs', '\\x89ÛÏsippin\\x89Ûª', '\\x89ÛÏplans', '\\x89ÛÏparties', '\\x89ÛÏnumbers', '\\x89ÛÏnobody', '\\x89ÛÏmake', '\\x89ÛÏmacdaddyleo', '\\x89ÛÏlordbrathwaite', '\\x89ÛÏlolgop', '\\x89ÛÏleoblakecarter', '\\x89ÛÏleejasper', '\\x89ÛÏkeits', '\\x89ÛÏi', '\\x89ÛÏfor', '\\x89ÛÏfdny', '\\x89ÛÏdylanmcclure55', '\\x89ÛÏdetonate\\x89Û\\x9d', '\\x89ÛÏcat', '\\x89ÛÏbbcwomanshour', '\\x89ÛÏbbcengland', '\\x89ÛÏbasedgeorgie', '\\x89ÛÏall', '\\x89ÛÏairplane\\x89Û\\x9d', '\\x89ÛÏ', '\\x89Û¢\\x89Û¢if', '\\x89Û¢im', '\\x89Û¢', 'zzzz', 'zxathetis', 'zurich', 'zumiez', 'zss', 'zrnf', 'zourryart', 'zotar50', 'zoom', 'zonewolf123', 'zonesthank', 'zones', 'zombies', 'zombiefunrun2014', 'zomatoaus', 'zojadelin', 'zodiac', 'zmne', 'ziuw', 'zippoline', 'zippers', 'zipper', 'zipped', 'ziphimup', 'zip', 'zionists', 'zimmerman', 'zimmer', 'zicac', 'zhenghxn', 'zhejiang', 'zeros', 'zero', 'zergele', 'zeno001', 'zenandemcfen', 'zehrs', 'zeal', 'zaynmaiikist', 'zarry', 'zarharzar', 'zar', 'zamtriossu', 'zaman', 'zakuun', 'zakbagans', 'zaibatsunews', 'zachzaidman', 'zach', 'zacb', 'zaatari', 'z3kesk1', 'yzf', 'yyj', 'yyeso', 'yycweather', 'yycfringe', 'yuvi', 'yuuko', 'yuppies', 'yup', 'yunita99', 'yumiko', 'yum', 'yukis', 'yug', 'yuan', 'ypg', 'yo\\x89Û', 'you\\x89Û\\x9d', 'you\\x89Û', 'youuu', 'youssefyamani', 'yourboyshawn', 'younoone', 'young\\x89Û', 'youngsafe', 'youngins', 'youngerampgrossly', 'younger', 'yosemite', 'yorkshire', 'yor', 'yonews', 'yolk', 'yolandaph', 'yogurt', 'yoga', 'yoenis', 'ymcglaun', 'ykelquiban', 'yiraneuni', 'yikes', 'yield', 'yiayplan', 'yhngsjlg', 'yh', 'yeyeulala', 'yessum', 'yeshayad', 'yennora', 'yemenis', 'yellows', 'yelllowheather', 'yelled', 'yell', 'yehuda', 'yeehaw', 'yeat', 'yeaahh', 'yday', 'ybtheprophet', 'yazidishingalgenocide', 'yard', 'yamashiro', 'yamaguchi', 'yahootv', 'yahoonewsdigest', 'yahoofinancehope', 'yahoocare', 'yahoo7', 'yahistorical', 'yagitudeh', 'yaboiluke', 'xylodemon', 'xxhjesc', 'xvii', 'xtra1360', 'xrays', 'xpost', 'xoxo', 'xojademarie124', 'xo', 'xmen', 'xmas', 'xl', 'xkdrx', 'xii', 'xhnews', 'xgninfinity', 'xfiles', 'xeni', 'xela', 'xekstrin', 'xdojjjj', 'xdescry', 'xboxone', 'xb1', 'xaviermarquis', 'xavier', 'x37bs', 'x2', 'x1441', 'x1434', 'x1411', 'x1402', 'x1392', 'x1386', 'w\\x89Û', 'wzbt', 'wyrmwood', 'wyou', 'wyattb23', 'wxky', 'wxii', 'wxiatv', 'wwwbigbaldhead', 'www', 'wwp', 'wwexdreamer', 'wwa', 'ww3', 'ww', 'wut', 'wugliness', 'wtwitter', 'wtony', 'wth', 'wtc', 'wsvr1686b', 'wsoc', 'wsoaring', 'wsls', 'wsjthinktank', 'wsj', 'wsazbrittany', 'wroug', 'wrote', 'wrongway', 'wrongperson', 'wrongdejavu', 'wrked', 'writingtips', 'writers', 'writebothfists', 'wristband', 'wrightsboro', 'wrestler', 'wrestleon', 'wreak', 'wrapped', 'wrap', 'wraith', 'wr', 'wqow', 'wpt994', 'wps', 'wpo', 'wowthe', 'wowsavannah', 'wout', 'wounded\\x89Û\\x9d', 'woundedpigeon', 'wouldelectrocute', 'worthless', 'worstoverdose', 'worstever', 'worship', 'worsen', 'worseits', 'worrying', 'worm', 'worldwatchesferguson', 'worldvision', 'worldpay', 'worldoil', 'worldnetdailyhomosexuality', 'worldlets', 'workspace', 'workd', 'wordpressdotcom', 'wordk', 'wording', 'wooooooo', 'woodward', 'woodland', 'wooden', 'woodchucks', 'woo', 'wonders', 'wonderousallure', 'wonderkid', 'wonderfully', 'wompppp', 'women\\x89Ûª', 'womengirls', 'womem', 'womb', 'woman\\x89Ûªs', 'wolter', 'wolforth', 'woken', 'woes', 'woah', 'wnw', 'wnukes', 'wniagospel', 'wn', 'wmiddle', 'wm', 'wld', 'wlandslide', 'wkrn', 'wknd', 'wk', 'wizard', 'wiwnpfxa', 'witnessing', 'witnesses', 'witnessed', 'withåÊannihilation', 'withstand', 'withering', 'withdrawur', 'withdraws', 'witch', 'wishlist', 'wishing', 'wished', 'wisely', 'wise', 'wisdomwed', 'wisdom', 'wisdc', 'wipes', 'winnipeg', 'winning', 'winner', 'winik', 'wingers', 'winged', 'wing', 'winechat', 'windwakerstyle', 'windstormåÊinsurer', 'windstormfollow', 'windsor', 'windows10', 'windowgatribble', 'windmy', 'windits', 'win10', 'wimp', 'wimbledon', 'wilsons', 'willow', 'willis', 'willinghearted', 'willieami', 'willian', 'willhillbet', 'wildwestsixgun', 'wildlooking', 'wildlionx3', 'wildlife', 'wildhorses', 'wilden', 'wikipedia', 'wii', 'wifi', 'wifekids', 'wiedemer', 'width', 'widout', 'wider', 'widda16', 'wickett', 'whyor', 'whvholst', 'who\\x89Û', 'whopperjr760', 'whod', 'whocares', 'whitt', 'whitewashes', 'whistled', 'whistleblower', 'whiskey', 'whippenz', 'whipe', 'whimsy', 'while\\x89Û', 'whereas', 'wher', 'whensoever', 'whelen', 'wheelsio', 'wheel', 'whedonesque', 'wheatley', 'whatevs', 'whatever', 'whatcanthedo', 'whashtag', 'whackamole', 'wha', 'wfries', 'wfaaweather', 'we\\x89Ûªve', 'we\\x89Ûªre', 'wexler', 'wews', 'wew', 'west\\x89Ûªs', 'westward', 'wests', 'westminister', 'westmarch', 'westerosnah', 'westeros', 'westerncanadadrought', 'westchester', 'wesleylowery', 'wesley', 'wereonadesolateplanet', 'weren\\x89Ûªt', 'wenger', 'wendell', 'welshninja87', 'weloveyoulouis', 'weloverobdyrdek', 'welovela', 'wells', 'wellknown', 'wellgrounded', 'welles7', 'welladjusted', 'welcomes', 'weirdo', 'weiqin', 'weights', 'weightless', 'weigh', 'weep', 'weeklong', 'weekends', 'weei', 'weebly', 'wednesday\\x89Û', 'wednes', 'wedneday', 'wedgie', 'weddinghour', 'websites', 'webinar', 'web', 'weatherstay', 'weatherit', 'wears', 'weaponxmusic', 'wealilknowa', 'weaknesses', 'weakness', 'wdyouth', 'wdym', 'wdtv', 'wderailed', 'wcw', 'wctv35', 'wccorosen', 'wc', 'wbu', 'wbre', 'wbcshirl2', 'wbc2015', 'waziristan', 'way\\x89Û\\x9dyeah', 'wayward', 'waynesteratl', 'wayi', 'wayfieldstone', 'wave\\x89Ûª', 'waved', 'wattys2015', 'wattle', 'water\\x89Û', 'watersafety', 'waterproof', 'waterfur', 'waterboarding', 'watchthevideo', 'watchout', 'watchin', 'watches', 'wasting', 'wastenoxious', 'wastelands', 'wasted', 'wasn\\x89Ûªt', 'wasnamp8217t', 'wasilla', 'washing', 'washard', 'waseembadami', 'war\\x89Û', 'warzone', 'wartime', 'warthen', 'warsgoddess', 'warriorcord', 'warranted', 'warra', 'warped', 'warningwild', 'warnings900037', 'warnerrobins', 'warned\\x89Û\\x9d', 'warmth', 'warmbodies', 'warlordqueen', 'warfare', 'wardens', 'ward', 'wantmyabsback', 'wanother', 'wank', 'wander', 'waltdisney', 'wall\\x89Û\\x9d', 'wallybaiter', 'wales', 'wakho', 'wakeupflorida', 'waiver', 'waited', 'waimea', 'wahpeton', 'wahhabism', 'wage\\x89Ûª', 'waferthin', 'wackos', 'wackoes', 'vÌdeo', 'v\\x89Û', 'vzwsupport', 'vvorm', 'vuzuhustle', 'vulnerability', 'vulnera', 'vuln', 'vtc', 'vroman', 'vra50\\x89Û\\x9d', 'votes', 'voters', 'vosloorus', 'vortex', 'voortrekker', 'voodooben', 'volunteers', 'volunteer', 'voltaire', 'volfan326', 'volcanotornado', 'volcanodiscover', 'volcanic', 'void', 'voices', 'vodka', 'vocals', 'vocalist', 'vocal', 'vmas', 'vj44', 'vi\\x89Û', 'vixstuart', 'vixmeldrew', 'vivid', 'vivianunhcr', 'viviangiang', 'vivian', 'vivaargentina', 'vitesse', 'vitaly', 'vitalvegas', 'vita', 'visting', 'visiting', 'visionzero', 'visible', 'visibility', 'visage', 'virtual', 'vir', 'viper', 'violin', 'violets', 'violentfeminazi', 'violators', 'violation', 'violated', 'vinustrip', 'vinnie', 'vines', 'vincent', 'vimvith', 'vimeo', 'villicanaalicia', 'villa', 'vilelunar', 'vikings', 'vigils', 'vigilent', 'views', 'viennabutcher', 'videogame', 'videoclip', 'victorious', 'victorias', 'victorian', 'victoriagittins', 'vickybrush', 'vichardy', 'vibrates', 'vibrate', 'viab', 'vhull', 'vhs', 'vgbootcamp', 'vets78734', 'vestment', 'vessels', 'versus', 'veronicadlcruz', 'vern', 'vermilion', 'verhoek', 'vergil', 'verge', 'verde', 'venture', 'ventilated', 'vent', 'venoms', 'veneto', 'veldfest', 'veld', 'veins', 'veil', 'veggies', 'vegetables', 'vegassolitude', 'vegan', 'veg', 'vector', 'vaxshill', 'vast', 'vassalboro', 'varagesale', 'vanpoli', 'vannuyscouncil', 'vanishing', 'vanished', 'vanilla', 'vanessas', 'vanessa', 'vandalized', 'vancouveråÊisland', 'vampires', 'value', 'valuations', 'valleywx', 'vallerand', 'valentines', 'vale', 'valdes1978', 'vail', 'vai', 'vaginaorcake', 'vagersedolla', 'vaccines', 'vaccine', 'vacancies', 'vaca', 'vabengal', 'v452', 'u\\x89Û', 'ux', 'uvopwz', 'uve', 'uv', 'utv', 'utp', 'utopian', 'utilized', 'utility', 'utica', 'utfire', 'utd', 'utc3km', 'utc20150806', 'utahgrizz', 'utahcanary', 'uswarcrimes', 'usw', 'uspacific', 'usmnt', 'ushanka', 'usg', 'usfs', 'useless', 'usdot', 'usbush', 'usatodaynfl', 'usat', 'usarmy', 'usamisan', 'usama', 'usagi', 'usage', 'urufusanragu', 'uruan', 'urself', 'urogyn', 'urine', 'uriminzok', 'urgentthere', 'urg', 'urbanisation', 'urbanfashion\\x89Û', 'urban', 'uranium', 'upwindstorm', 'upward', 'uptownjorge', 'uptown', 'uptotheminute', 'upstairs', 'upsetting', 'uprooting', 'uprootin', 'uplifting', 'upi', 'uphill', 'upgrades', 'updateme', 'upcoming', 'upah', 'upa', 'unwomen', 'unwarranted', 'unwanted', 'unu', 'unto', 'untill', 'untameddirewolf', 'unsurprisingly', 'unsure', 'unsuccessful', 'unstoppable', 'unstable', 'unsigned', 'unsensibly', 'unsecured', 'unrecognized', 'unrealtouch', 'unreal', 'unprepared', 'unpredictable', 'unplug', 'unpacked', 'unnewsteam', 'unloads', 'unlicensed', 'unknowingly', 'univsfoundation', 'universityoflaw', 'uniteblue', 'unite', 'uninvestigated', 'unimpressed', 'unimaginable', 'unhinged', 'unhealed', 'unharmed', 'unhappy', 'ungodly', 'unfortunemelody', 'unfollow', 'unfold', 'unfml', 'unfair', 'unexplainable', 'unending', 'undone', 'undetected', 'undeserving', 'underwritersenior', 'underwriter', 'understood', 'understand\\x89Û\\x9d', 'understandable', 'underpasses', 'undermined', 'undergroundrailraod', 'undergroundbestsellers', 'underground', 'underestimate', 'uncover', 'uncontrolled', 'uncontrollable', 'unconsciously', 'unconscious', 'unconditional', 'uncommon', 'uncomfortable', 'uncles', 'uncertaintyeconomic', 'uncertain', 'unbelievably', 'unaware', 'unauthorized', 'unarmed', 'unaddressed', 'unable', 'umntu', 'umm', 'umbrella', 'um', 'uluru', 'ultimatum', 'ullman', 'uk\\x89Ûªs', 'ukraines', 'uknews', 'ukfrance', 'ukfloods', 'uhmmmm', 'uhhhhh', 'uglypeople', 'uglyamesocialaction', 'ugliest', 'ugc', 'ufo4ublogeurope', 'ufn', 'udom', 'udhampuragain', 'uchicago', 'uabstephenlong', 'u2', 'tÌüp', 't\\x89Û', 'tyrone', 'tyrant', 'typography', 'typing', 'typical', 'typhoon\\x89Û', 'typewriter', 'tyleroakley', 'tyar', 'txt', 'txlege', 'tx', 'twx', 'twoptwips', 'twoout', 'twitsandiego', 'twitch', 'twist', 'twill', 'twilights', 'twi', 'twentysix', 'tweetstorm', 'tweetinglew', 'tweeting', 'tweeted', 'tweet4taiji', 'tween', 'twcnews', 'twain', 'tvshowtime', 'tvjnews', 'tutorials', 'tusky', 'turner', 'turnedonfetaboo', 'turdnado', 'turbojet', 'tunisian', 'tunisia', 'tunis', 'tuneswgg', 'tuned', 'tunas', 'tumblr', 'tumbling', 'tumbles', 'tulowitzki', 'tullamarine', 'tuicruises', 'tuffers', 'tuesdays', 'tucson', 'tub', 'tu', 'ttw', 'tthe', 'tsutomi', 'tsunamis', 'tsipras', 'tshirts', 'tshirt', 'trynna', 'truthsof', 'trustymclusty', 'trusting', 'trusted', 'trulystings', 'truestory', 'truediagnosis', 'truckload', 'truckcrash', 'tru', 'trpreston01', 'trp', 'troyslaby22', 'troylercraft', 'troye', 'troupe', 'troubleonmymind', 'trophyhunt', 'trophy', 'trophies', 'tropes', 'trooper', 'trombonetristan', 'trollkrattos', 'trollingtilmeekdiss', 'troisrivieres', 'trjdavis', 'trixiedrowned', 'trivium', 'triumphs', 'triumphant', 'triumph', 'trinna', 'trim', 'trillac', 'trigger', 'trident', 'tricycle', 'tricky', 'trickxie', 'trickshot', 'trickier', 'triciaoneillphoto', 'triciaoneill', 'tribez', 'tribe', 'trib', 'triangle', 'triad', 'treyarch', 'trey', 'trestle', 'trend\\x89Û\\x9d', 'trends', 'trending', 'trench\\x89Û', 'tren', 'tremors', 'tremont', 'tremblayeh', 'trekkers', 'treescape', 'treblinka', 'treatmen', 'treasures', 'treasurehouse', 'trc', 'tray', 'travellers', 'travelelixir', 'trash', 'traplord29', 'transwomen', 'transportation', 'transporta', 'translated', 'transgress', 'transgendered', 'transgender', 'transcription', 'trampling', 'traitor', 'traintragedy', 'trained', 'trailheads', 'trailed', 'trail', 'trafficnetwork', 'traditionalist', 'trader', 'trade', 'tradcatknight', 'tracy', 'tractor', 'tracklist', 'tracey', 'trace', 'tra', 'tps', 'tprimo24', 'to\\x89Û', 'tozlet', 'toyota', 'toxicsavior', 'toxiccancerdiseasehazardous', 'toxic', 'towns', 'towing', 'tower\\x89Ûª', 'tower', 'towboat', 'tow', 'tours', 'tournaments', 'tournament', 'tourists', 'tounge', 'toughens', 'touching', 'touchdown', 'tottenham', 'totteham', 'totoooooooooo', 'totoooooo', 'totalitarianism', 'totalitarian', 'tosu', 'toss', 'tos', 'tory', 'tort', 'torso', 'torrential', 'torrent', 'torrecilla', 'torrance', 'torontorc', 'tornadogiveaway', 'tormented', 'tories', 'tora', 'topic', 'topdown', 'top25', 'tootrue', 'toosoon', 'toooooo', 'tookitlikeaman', 'tookem', 'toocodtodd', 'tonysandos', 'tonymcguinness', 'tonyhsieh', 'tonycottee1986', 'tonyburke', 'tonyabbottmhr', 'tony', 'tonne', 'tonight\\x89Ûªs', 'tonguetwister', 'ton', 'tomorrow\\x89Ûªs', 'tommorow', 'tomlinson', 'tomislav', 'tomfromireland', 'tomdean86', 'tomclancy', 'tolled', 'tolewantg', 'tolerated', 'tolerance', 'tokteacher', 'toilets', 'toiindianews', 'togthe', 'toes', 'toenail', 'toe', 'toddyrockstar', 'toddstarnes', 'toddcalfee', 'today\\x89Ûªs', 'todaythat', 'todayngr', 'todayng', 'todayim', 'todayhave', 'today4got', 'tod', 'tobiasellwood', 'tnwx', 'tnn', 'tneazzy', 'tna', 'tn', 'tms7', 'tmake', 'tlvfacesauspol', 'tlvfaces', 'tloz', 'tlk', 'tkyonly1fmk', 'tjrobertson2', 'tj', 'titty', 'tittie', 'titortau', 'titolo', 'titania', 'titadom', 'tita', 'tirelessly', 'tire', 'tipster', 'tip', 'tinybaby', 'tiny', 'tinted', 'ting', 'tindering', 'tinderbox', 'tinder', 'timmicallef', 'timing', 'time\\x89Û\\x9d', 'timesofindia', 'timesap', 'timed', 'timebomb', 'timber', 'timaroberts', 'tightly', 'tight', 'tiggr', 'tigersjostun', 'tiffanyfrizzell', 'tier', 'tide', 'tidalhifi', 'ticklemeshawn', 'tianta', 'th\\x89Û', 'thursdays', 'thursd', 'thurs', 'thurlow', 'thunderstormtornado', 'thundersnow', 'thugging', 'thucydiplease', 'tht', 'thruuu', 'thrusts', 'throwin', 'thriving', 'threesome', 'threealarm', 'threat\\x89Ûª', 'threatintel', 'threatconnect', 'thread', 'thrarchives', 'thr', 'thoutaylorbrown', 'thoughwill', 'thou', 'thoroughly', 'thorium', 'thorins', 'thorgan', 'thomassmonson', 'thomashcrown', 'thnk', 'thi\\x89Û', 'this\\x89Û', 'thisispublichealth', 'thisisperidot', 'thisishavehope', 'thisisfaz', 'thisdayinhistory', 'thirtyfive', 'thirdquarter', 'thinner', 'thinkpink', 'thingsihate', 'thh', 'they\\x89Ûªd', 'thexfiles201days', 'thewesterngaz', 'thetxi', 'thetshirtkid', 'thetimepast', 'thestrain', 'thesmallclark', 'thesewphist', 'thesensualeye', 'theresmorewherethatcamefrom', 'thereof', 'thereisonlysex', 'therein', 'therefore', 'therealrittz', 'thereal', 'theramin', 'thepartyofmeanness', 'then\\x89Û', 'thenissonian', 'thenewshype', 'theneeds', 'themhe', 'themermacorn', 'theme', 'themalemadonna', 'themaine', 'thelegendblue', 'thejonesesvoice', 'thejenmorillo', 'theirs', 'thehobbit', 'thehammers', 'thegreenparty', 'theghostparty', 'theevilolives', 'theemobrago', 'theellenshow', 'theeconomist', 'thedoolinggroup', 'thedayct', 'thedarktower', 'thedailyshow', 'thedailybeast', 'thecomedyquote', 'theburnageblue', 'thebriankrause', 'theboyofmasks', 'thebookclub', 'theblackshag', 'thebargain', 'thebachelorette', 'theatres', 'theatlantic', 'theatershooting', 'theashes', 'theadvocatemag', 'thda', 'that\\x89Û', 'thatwitchem', 'thatswhatfriendsarefor', 'thatsabinegirl', 'thatrussianman', 'thatpersianguy', 'thatnot', 'thatfatguy', 'thatdes', 'thatd', 'thankyou', 'thanku', 'thankkk', 'thanking', 'thalapathi', 'thailand', 'thai', 'tfw', 'tfb', 'te\\x89Û', 'texture', 'texts', 'texting', 'texaschainsawmassacre', 'texans', 'testy', 'testify', 'testified', 'testicles', 'tesco', 'terwilliger', 'tersestuff', 'territory', 'terrified', 'terrific', 'terrain', 'termn8r13', 'terminated', 'terell', 'tepat', 'tents', 'tenshi', 'tennis', 'tennews', 'tends', 'ten4', 'tem\\x89Û', 'temps', 'temporary', 'temporarily', 'templates', 'temperature', 'temp', 'temecula', 'temecafreeman', 'telnet', 'tellyfckngo', 'tellyampi', 'telly', 'telltales', 'teleported', 'telemarketing', 'telekinesis', 'telegraphworld', 'telangana', 'tee\\x89Û', 'teenfiction', 'teena797', 'teemo', 'tee', 'teduka', 'tedcruz2016', 'tecno', 'techniqu', 'technical', 'technews', 'techesback', 'team\\x89Û', 'teamvodg', 'teamsurvivors', 'teamscorpion', 'teamo', 'teammates', 'teamhennessy', 'teamfollowback', 'teamatowinner', 'teahivetweets', 'teafrystlik', 'tdog', 'tdm', 'tcotåÊccot', 'tcgreno', 'tbs', 'tbr', 'tblack', 'tbh', 'taylorswift13', 'taylors', 'taykreidler', 'tayiorrmade', 'taxstone', 'taxreturn', 'taxpayers', 'taxis', 'taxi', 'taxes', 'tawfmcaw', 'taungbazar', 'taufikcj', 'tattoos', 'tattooed', 'tat', 'tastemycupcakee', 'tasks', 'task', 'tarzana', 'tarynel', 'tarp', 'targe', 'tareksocal', 'taraswart', 'tapas', 'taoistinsight', 'tantrums', 'tanstaafl23', 'tanslash', 'tangletalk', 'tangled', 'tanehisicoates', 'tampons', 'tampabay', 'tambourine', 'tambo', 'tallest', 'talkradio', 'talkinghell', 'talked', 'talkecologyamphuman', 'talisman', 'talibans', 'taliban', 'tales', 'takis', 'takin', 'takeoff', 'takehome', 'takecare', 'takeaways', 'taipei', 'tailor', 'tail', 'tahoeblazeravalanches10', 'tagging', 'tagged', 'tafs', 'tae', 'tadhgtgmtel', 'tactics', 'tactful', 'tacos', 'tackettdc', 'tacit', 'taaylordarr', 't1000s', 's\\x89Ûªarabia', 'szuter', 'szmnextdoor', 'systematic', 'sys', 'syringetoanger', 'synapsenkotze', 'symptoms', 'symbol', 'symantec', 'syjexo', 'sydtraffic', 'swtrains', 'sworn', 'swords', 'swooping', 'swollen', 'swivels', 'switzerland', 'switching', 'switch', 'swiss', 'swingman', 'swiming', 'swiftycommissh', 'swiftly', 'swellyjetevo', 'swell', 'sweets', 'sweetpeas', 'sweetiebirks', 'sweeps', 'sweeping', 'sweep', 'swedish', 'sweaty', 'sweatfyi', 'sweated', 'swea', 'swb1192', 'swayoung01', 'swayback', 'swanger', 'swami', 'swag', 'svetlana', 'sux', 'suvs', 'sustainourearth', 'sustainability', 'suspicious', 'suspended', 'suspects', 'susiya', 'susinesses', 'sushi', 'susanj357', 'survivorsr', 'surviving', 'surveys', 'suruÌ¤', 'surgical', 'surges', 'surfspa', 'surfphoto', 'surety', 'sureshpprabhu', 'suresh', 'suregod', 'supremo', 'supremacist', 'supposedly', 'supporting', 'supporthealthhomebathroomsupportelderlyinjureds\\x89Û', 'supporters', 'superv', 'superstitions', 'superstition', 'superpower', 'supernovalester', 'supernatural', 'supermarket', 'superman', 'superiority', 'superintendent', 'superintende', 'superfood', 'superbug', 'superb', 'sunshine', 'suns', 'sunrays', 'sunnymeade', 'sunk\\x89Û1', 'sunflower', 'sundercr', 'sunday\\x89Ûªs', 'sundaydont', 'sunburst', 'sunburned', 'sunbathe', 'sumo', 'sumn', 'summons', 'summon', 'summervibes', 'summers', 'summerhallery', 'summer2k15', 'summary', 'sultry', 'sul', 'suites', 'suited', 'suitable', 'suing', 'suicides', 'suicidebycop', 'suho', 'suggs', 'sugar', 'suffield', 'sufficiently', 'suffers', 'suelinflower', 'sued', 'sudan\\x89Ûªs', 'sucking', 'suckers', 'sucked', 'suck', 'succeed', 'subtornado', 'subtlety', 'subtle', 'substantial', 'substance', 'subsequent', 'subsd', 'subscription', 'submitt', 'submissions', 'submerged', 'subjected', 'subcontractor', 'subconscious', 'subcommittee', 'subatomic', 'su', 'stylist', 'stylishly', 'styled', 'stvmlly', 'stury', 'sturgis', 'stupidniggr', 'stunningly', 'stung', 'stunckle', 'stump', 'stuffin', 'studying', 'studebaker', 'studded', 'stuckinbooks', 'stuartbroad8', 'strutting', 'strutted', 'struggle', 'structuring', 'strongminded', 'strives', 'striptease', 'strips', 'stripped', 'stripe', 'strip', 'striked', 'strictly', 'strickskin', 'stretches', 'stretcherbearers', 'stretcherbearer', 'stretched', 'stressing', 'stresses', 'strength', 'streetlight', 'streetjamzdotnet', 'stree', 'streamyxhomesouthern', 'streams', 'streaming', 'stray', 'strawberrysoryu', 'strawberries', 'stratford', 'strategyhua', 'strategies', 'strap', 'strangers', 'strand', 'straits', 'strains', 'strain', 'straighten', 'sto\\x89Û', 'stowing', 'stormtrooper', 'stormlike', 'storming', 'stormfree', 'stormbeard', 'storey', 'storen', 'stop\\x89Û\\x9d', 'stopping', 'stoponesounds', 'stopharper', 'stopevictions', 'stood', 'stony', 'stonewall', 'stones', 'stonebrewingco', 'stokes', 'stockwell', 'stockholm', 'sto', 'stlouis', 'stlnd', 'stl', 'stirring', 'stil', 'stiiilo', 'stickynyc', 'sticky', 'sticking', 'sticker', 'sthing', 'stfxuniversity', 'stevie', 'steveycheese99', 'stevenrulles', 'stevenontwatter', 'steven', 'sterotypical', 'stern', 'sterlingscott', 'sterlingknight', 'sterling', 'stepkans', 'stephenson', 'stephenscifi', 'stephenking', 'stephengeorg', 'stephaniemarija', 'steph93065', 'stemming', 'stem', 'stefano', 'stefanejones', 'steep', 'steellord', 'stearns', 'stealth', 'steady', 'stds', 'staying', 'stayed', 'stavola', 'stavernise', 'statistically', 'stations', 'stationcdrkelly', 'states\\x89Û', 'statesville', 'statements', 'stat', 'starving', 'starve', 'startrek', 'startide', 'startelegram', 'starmade', 'starks', 'stark', 'starflamegirl', 'stare', 'stardate', 'starbuckscully', 'starbs', 'stankyboy88', 'standup', 'standstill', 'standforwolves', 'standardised', 'standardanonymous', 'stallion150', 'stalled', 'stalins', 'stalag', 'staining', 'staid', 'stages', 'staged', 'staff\\x89Û', 'staffing', 'stacy', 'stacks', 'stack', 'stacedemon', 'stacdemon', 'stable', 'ssw', 'ssu', 'ssssnell', 'ssshhheeesshh', 'ssp', 'ssb4', 'srsly', 'srs', 'srk', 'sriramk', 'srajapakse', 'sr37', 'sr22', 'sr14', 'sqwizzix', 'squirrel', 'squibby', 'squeezed', 'squeaver', 'squeaky', 'squabble', 'sq', 'spyro', 'spying', 'spx', 'sputtering', 'sputnikint', 'spurs', 'spurgeon', 'spur', 'spså¨', 'spsgsp', 'sprite', 'sprinklers', 'springer', 'spree', 'spreads', 'spread', 'spray', 'sprains', 'spouting', 'spouse', 'spotting', 'spotlight', 'sportsroadhouse', 'sportinggoods', 'sporten', 'spookyfob', 'sponsor', 'sponge', 'spokes', 'spoke', 'spokane', 'spoiled', 'spoil', 'splifs', 'splattershot', 'splatoon', 'splatling', 'splatdown', 'splash', 'spits', 'spit', 'spirits', 'spins', 'spinnellii', 'spin', 'spilt', 'spillevacuationsred', 'spike', 'spies', 'spiderweb', 'spicybreads', 'spice', 'spends', 'spencerfearon', 'spen', 'spells', 'spell', 'speedtech', 'speeding', 'speech', 'speculation', 'speculatio', 'spectrum', 'specs', 'specifically', 'specialized', 'specialize', 'speccy', 'speakingfromexperience', 'sparxxx', 'spartans', 'sparkz', 'sparking', 'spark', 'spaniels', 'spaniel', 'span', 'spammers', 'spam', 'spacex', 'spacewolverine', 'spaceshiptwo', 'spaceangelseven', 'so\\x89Û', 'soz', 'sow', 'south\\x89Û', 'southwestern', 'southwest', 'southridgelife', 'southline', 'southkorea', 'southdown', 'southbound', 'southaccident', 'sousse', 'sourmashnumber7', 'sour', 'soundtrack', 'sounding', 'sounders', 'soultech', 'sought', 'souda', 'sothwest', 'sos', 'sorryi', 'sorrybutitstrue', 'sorrowful', 'sorrower', 'sorrow', 'sorely', 'sophistication', 'sophiewisey', 'sophieingle01', 'soonpandemonium', 'soonergrunt', 'sonyprousa', 'sony', 'sonoranrattler', 'sonofbobbob', 'sonofbaldwin', 'sonisoner', 'sonia', 'soni', 'songhey89', 'songfor', 'soner', 'sond', 'sona', 'somme', 'sometimesi', 'somethin\\x89Ûª', 'somethingyr', 'someday', 'solving', 'solelinks', 'sole', 'soldi', 'solano', 'sojapan', 'soil', 'soggy', 'softball', 'sofa', 'sods', 'sodamntrue', 'sockets', 'socket', 'sock', 'socialwots', 'socialtimes', 'socialmediadriven', 'socially', 'socal', 'soc', 'sobbing', 'soapscoop', 'soap', 'soaker', 'soaked', 'snuck', 'snowywolf5', 'snowy', 'snowstormhailstorm', 'snowstormdespite', 'snowflake', 'snotgreen', 'snort', 'snoop', 'snooker', 'snippets', 'snipe', 'sniiiiiiff', 'sniff', 'sni', 'sneezing', 'sneaks', 'snazzychipz', 'snapping', 'snapharmony', 'snapchatselfie', 'snakes', 'snake', 'snack', 'sn', 'smusx16475', 'smugglersåÊnabbed', 'smugglers', 'smug', 'smth', 'sms087809233445', 'sms', 'smp', 'smores', 'smoothed', 'smoochy', 'smokey', 'smokes', 'smokers', 'smoakqueen', 'smirking', 'smiles', 'smfh', 'smem', 'smelltaste', 'smelling', 'smelled', 'smeared', 'smartteks', 'smartnews', 'smart', 'smantibatam', 'smallforestelf', 'smaller', 'smallbusiness', 'smallbiz', 'smack', 'slums', 'slumber', 'slsp', 'slsandpet', 'slows', 'slowpoke', 'slosheriff', 'slosher', 'slopeofhope', 'slone', 'slogan', 'slit', 'slipping', 'slipper', 'slimebeast', 'slightest', 'slight', 'sliding', 'slideshare', 'sliced', 'slew', 'slept', 'sleepjunkies', 'slay', 'slaves', 'slavery', 'slave', 'slaughter', 'slatukip', 'slating', 'slated', 'slashandburn', 'slapping', 'slander', 'slams', 'slamming', 'slam', 'slain', 'slabs', 'sl', 'skyåÊnews', 'skywars', 'skywarn', 'skyscrapers', 'skyrim', 'skype', 'skynews', 'skynet', 'skyler', 'skylanders', 'skull', 'skippy6gaming', 'skip', 'skinless', 'skims', 'skimmed', 'skiing', 'skies', 'ski', 'skh', 'sketchbook', 'skeleton', 'skc', 'skateboards', 'skarletan', 'skardu', 'skaggs', 'sk398', 'sjubb', 'sj', 'sizygwwf', 'sizewell', 'sixcar', 'sivan', 'situ', 'sittway', 'siteinvestigating', 'sister\\x89Ûª', 'sirtophamhat', 'sirtitan45', 'sirmixalot', 'sirmione', 'sirius', 'sirenvoice', 'sirens\\x89Û', 'sirensong21', 'sirensamp', 'sippin', 'sip', 'siouxland', 'siouxlan', 'sioux', 'sins', 'sinkingshipindy', 'sinkingfund', 'sinkhole\\x89Û', 'sinistras', 'singled', 'singlecar', 'sindh', 'sincerely', 'since3g', 'since1970the', 'simulation', 'simulating', 'simplify', 'simmons', 'similar', 'silvery', 'silverwood', 'silverman', 'silverhusky', 'silo', 'silly', 'silinski', 'silentmind', 'silent0siris', 'silenced', 'silas', 'sikh', 'sigue', 'signin', 'significance', 'signatureschange', 'sigh', 'sifting', 'siena', 'sidjsjdjekdjskdjd', 'side\\x89Û', 'sidewalk', 'sidelinesavage', 'sided', 'sick\\x89Ûª', 'sibling', 'shuts', 'shunichiro', 'shuffled', 'shuffle', 'shud', 'shtf', 'shtap', 'sht', 'shrews', 'showwent', 'showersstorms', 'showers', 'showdown', 'shovel', 'shove', 'shoutout', 'shouted', 'shouout', 'shotgun', 'shorts', 'shortfalls', 'shopping', 'shoppe', 'shootoutåÊ', 'shootings', 'shoook', 'sholt87', 'shocking\\x89ÛÏ', 'shocking', 'shoalstraffic', 'shiver', 'shitton', 'shite', 'shirley', 'shipsxanchors', 'shimmyfab', 'shii', 'shifts', 'shifter', 'shifted', 'shield', 'shidddd', 'shias', 'shia', 'she\\x89Ûªs', 'shevlinhixon', 'shestooyoung', 'sherfield72', 'shen', 'shemesh', 'sheltersupport', 'shells', 'shekhargupta', 'sheeting', 'shedid', 'shear', 'shayoly', 'shawie17shawie', 'shattered', 'shatter', 'sharply', 'sharper', 'shark\\x89Û\\x9d', 'shark', 'sharif', 'sharia', 'shar', 'shaping', 'shaper', 'shapeand', 'shaolin', 'shantaeskyy', 'shantaehalfgeniehero', 'shantaeforsmash', 'shanghai\\x89Ûªs', 'shanaynay', 'shakjn', 'shakingcatching', 'shakespeares', 'shakeology', 'shaken', 'shake', 'shaheed', 'shadows', 'shadowman', 'shadowflame', 'shadowed', 'shade', 'shad', 'sha', 'sgc72', 'sg', 'sfor', 'sfgiants', 'sfa', 'sexydragonmagic', 'sexuality', 'sexist', 'sewing', 'seward', 'sewage', 'severing', 'severely', 'seventies', 'sevenfold', 'sevenfigz', 'setting4success', 'setsuko', 'sethalphaeus', 'setanta', 'sessions', 'session', 'serving', 'servicin', 'servicesft7p7a', 'sergiopiaggio', 'serephina', 'serene', 'serbian', 'seras', 'sequence', 'sequalae', 'septic', 'separation', 'separated', 'senzu', 'sentient', 'sentenced', 'sensory', 'sensorknock', 'sensitive', 'sensei', 'senschumer', 'sensanders', 'sens', 'senfeinstein', 'senators', 'senatemajldr', 'sen', 'seminars', 'semi', 'semasirtalks', 'selmoooooo', 'selmo', 'selfseeking', 'selfpity', 'selfinflict', 'selfesteem', 'selfdestruction', 'selfdelusion', 'selfavowed', 'selects', 'select', 'sel', 'sejorg', 'seizing', 'seize', 'seismicsoftware', 'seismicresistant', 'segment', 'segas', 'sef', 'seeyouatamicos', 'seeweed', 'seemly', 'seemeth', 'seeker', 'seeds', 'seed', 'sedar', 'sedan', 'securing', 'secures', 'securedgt', 'secured', 'sectors', 'sections', 'secondhand', 'seclusion', 'sec', 'seaworld', 'seattletimes', 'seattles', 'seattledot', 'seats', 'seatbelt', 'seasonfrom', 'seashore', 'seas', 'seanhannity', 'seagulls', 'seagull07', 'sd', 'scynic1', 'scwx', 'scum', 'scuf', 'sct014', 'sct012', 'scseestapreparando', 'scrolling', 'scriptettesar', 'screwed', 'screenshot', 'screening', 'screeching', 'screamsdont', 'scratching', 'scratches', 'scraptrident', 'scraped', 'scrambledeggs', 'scouts', 'scout', 'scourgue', 'scourge', 'scotto519', 'scottdpierce', 'scotrail', 'scotiabank', 'scored', 'scorched', 'scofield', 'scmpnews', 'scissor', 'sciencefiction', 'scichat', 'schulz', 'schoolboy\\x89Ûªs', 'scholars', 'schoenfeld', 'schism\\x89Ûª', 'schelbertgeorg', 'scheer', 'scenes', 'scenario', 'scegnews', 'scasualty', 'scaryeven', 'scarlet', 'scariest', 'scarier', 'scandals', 'scandal', 'scam', 'scalpium', 'scaligero', 'scabs', 'sbee', 'sa\\x89Û', 'say\\x89Û\\x9d', 'saynae', 'sayin', 'sayedridha', 'savs', 'savour', 'savior', 'saveti', 'saves', 'saver', 'savedenaliwolves', 'savannahross4', 'savages', 'savagenation', 'saumur', 'sauldale305', 'saudiåÊmosque', 'saudies', 'saudiarabia', 'saturation', 'saturated', 'satoshis', 'satisfying', 'satire', 'satin', 'satellites', 'satans', 'satanaofhell', 'sask', 'sasha', 'sarumi', 'sarniamakchris', 'sarcastic', 'sarahmcpants', 'sarahksilverman', 'sara', 'sapphirescallop', 'san\\x89Ûªa', 'santos', 'santiago', 'santanicopandemonium', 'santaclara', 'sansa', 'sanonofre', 'sanity', 'sanitizing', 'sanitised', 'sang', 'sanfrancisco', 'sanford', 'sanelesstheory', 'sandwich', 'sandunes', 'sandra', 'sanders', 'sandbox', 'sanction', 'sanchez', 'samsung', 'samsmithworld', 'samples', 'sample', 'sammysosita', 'sammy', 'samihonkonen', 'sami', 'samelsamel', 'samaritans\\x89Ûª', 'sam', 'salyersblairhall', 'salvages', 'salvadors', 'salvadoran', 'salute', 'salty', 'saltriverwildhorses', 'salted', 'salopek', 'salmanmydarling', 'salman', 'sally', 'salisbury', 'sales', 'salado', 'saladinahmed', 'sakuuchiha', 'saku', 'sakhalintribune', 'saison', 'saintsfc', 'saint', 'sailors', 'safyuan', 'safsufa', 'safferoonicle', 'safes', 'saferåÊ', 'safeco', 'safaris', 'safari', 'sadtraumatised', 'saddledome', 'saddle', 'sacrifice', 'sackville', 'sackings', 'sabotagei', 'sabcnewsroom', 'saat', 'saalon', 'saadthe', 's61231a', 's5', 's3xleak', 's01e09', 'rzimmermanjr', 'ryt', 'ryrotheunaware', 'ryleedowns02', 'ryans', 'ryanoss123', 'rwrabbit', 'rvfriedmann', 'rvaping101', 'rvacchianonydn', 'rv', 'ruthann', 'russiaukraine', 'russellville', 'russell', 'russaky89', 'rushlimbaugh', 'rural', 'rupaul', 'run\\x89Û', 'runnin', 'runners', 'runkeeper', 'runjewels', 'runin', 'runaway', 'runabout', 'rumor', 'rumbling', 'rumah', 'rum', 'ruling', 'ruler', 'ruled', 'ruhl', 'rude', 'ruddyyyyyy', 'rubybot', 'rubi', 'rubbing', 'rubbin', 'rubbery', 'ru', 'rtsampdemocracy', 'rtrrtcoach', 'rtirishirr', 'rtcom', 'rsx', 'rstormcoming', 'rspca', 'rslm72254', 'rsf', 'rsa', 'rs5', 'rs40000cr', 'rrusa', 'rq', 'rpn', 'rp', 'roy', 'rowysolouisville', 'rowyso', 'rowaa', 'roving', 'routing', 'routine', 'routes', 'router', 'rousey', 'roundhouse', 'round2', 'rotting', 'rottentomatoes', 'rotations', 'rotation', 'rotating', 'rotary', 'rosters', 'roster', 'rossmartin7', 'rossbarton', 'roskomnadzor', 'rosewell', 'rosenthalauthor', 'rosenbergs', 'rosemarytravale', 'rorington95', 'ropes', 'roomsgrrrr', 'rooms', 'roomr', 'rooftops', 'roofing', 'roofers', 'ronwyden', 'ronincarbon', 'ronge', 'rondarousey', 'ronda', 'ronald', 'romp', 'romford', 'romes', 'romeocrow', 'romeo', 'romanticsuspense', 'romantic', 'romania', 'romanatwoodvlogs', 'roman', 'rom', 'rolo', 'rolling\\x89Û', 'roles', 'roleplay', 'rolandonabeats', 'rokiieee', 'roh3smantibatam', 'roguewatson', 'rogers', 'roga', 'rods', 'rodkiai', 'roddypiperautos', 'rodarmer21', 'rockstar', 'rockn', 'rockingham', 'rocking', 'rochdale', 'robthieren', 'robsimss', 'robpulsenews', 'robotlvl', 'robotcoingame', 'robot', 'roblox', 'robertwelch', 'robertoneill31', 'robertmeyer9', 'robertharding', 'robertcalifornia', 'robertbenglunds', 'robdelaney', 'robbiewilliams', 'robbed', 'roar', 'roanoketimes', 'roadworks', 'roadwayproperty', 'roadid', 'rnk', 'rlyeh', 'rlauren83199', 'rjkrraj', 'rjg0789', 'rjailbreak', 'rizzo', 'rivers', 'riverroaming', 'riveeeeeer', 'rivals', 'ritzyjewels', 'ritualistic', 'ritual', 'rite', 'risky', 'riser', 'ris', 'ririnsider', 'rips', 'ripriprip', 'ripples', 'ripping', 'rioters', 'rioslade', 'riooooos', 'rio2016', 'rinkydnk2', 'rindou', 'rin', 'rijn', 'rigour', 'rightly', 'righteous', 'rigga', 'rig', 'rifle', 'ridiculously', 'riddler', 'ricotta\\x89Û', 'rico', 'rickybonessxm', 'rickets', 'ricin', 'richhomeydon', 'riches', 'richelieusaintlaurent', 'richarkkirkarch', 'riceechrispies', 'rice', 'ribbon', 'ri', 'rhymes', 'rhinestone', 'rhiannon', 'rhett', 'rhee1975', 'rgj', 'rfp', 'rfcgeom66', 'rezaphotography', 'reworked', 'rewatchingthepilot', 'revolutionblight', 'revolt', 'revitup', 'revise', 'reviewing', 'reverse', 'reversal', 'revere', 'revenge', 'revel', 'reveillertm', 'revealing', 'reusing', 'reunite', 'retweeted', 'returning', 'retroactive', 'retreat', 'retract', 'retooled', 'retirement', 'retirees', 'retiredfilth', 'retard', 'retainers', 'resumed', 'restrospect', 'restoringpaths', 'restoring', 'restlessness', 'resting', 'restart', 'resque', 'responding', 'respondents', 'respects', 'respecting', 'resource', 'resort', 'resolved', 'resolutevanity', 'resoluteshield', 'resistant', 'resin', 'resilience', 'resigninshame', 'residualincome', 'residual', 'reshrimplevy', 'reshareworthy', 'reshape', 'reset', 'reserves', 'reserved', 'resemblance', 'researchers', 'rescuing', 'rescuersthe', 'rescued\\x89Û', 'rescuedagain', 'rescind', 'requiring', 'requiem', 'requests', 'requa', 'reqd', 'reputation', 'reps', 'reprocussions', 'reprises', 'repression', 'represents', 'representing', 'representative', 'repped', 'reporters', 'replacement', 'replaced', 'repjohnkatko', 'repdonbeyer', 'repatriating', 'reoccur', 'rent', 'renovation', 'renewsit', 'renewed', 'renew911health', 'rene', 'rendered', 'render', 'renaomino', 'renamed', 'ren', 'remymarcel', 'remote', 'remorseless', 'remodeled', 'remixes', 'reminders', 'reminded', 'remind', 'remembrance', 'remembers', 'rememberrabaa', 'remedial', 'rembr', 'remaster', 'remarkably', 'remark', 'remand', 'remainontop', 'remaining', 'remade', 'relive', 'religious', 'reliefweb', 'relevance', 'relegation', 'releasing', 'relaxinpr', 'relations', 'rejoice', 'rejectdcartoons', 'reiterate', 'reined', 'reince', 'reimagining', 'reigncoco', 'reidlake', 'regress', 'regr', 'regime', 'reggaeboyz', 'regent', 'regc', 'reg', 'refuses', 'refunds', 'refund', 'refugees\\x89Û', 'refugeesmatter', 'reflects', 'reflections', 'reflected', 'referred', 'referencereference', 'reference', 'refer', 'reeves', 'reef', 'reed', 'reebok', 'redwing', 'reduces', 'redskins', 'reds', 'redlands', 'redistribute', 'rediscovered', 'rediscover', 'redhead', 'redhanded', 'redesigning', 'redesigned', 'redemption', 'redeemer', 'redeem', 'reddish', 'reddevil4life', 'reddakushgodd', 'redcliffe', 'redbull', 'redblood', 'recruitment', 'recruiting', 'recovered', 'recordhigh', 'recorded', 'recordand', 'reconnect', 'recommendations', 'recoil', 'recognition', 'recognised', 'recluse', 'reckless', 'recipe', 'recip', 'receives', 'recalled', 'recal', 'rec', 'rebound', 'reboot', 'reblogged', 'rebelmage2', 'rebelled', 'rebecca', 'rebahes', 'reassigned', 'realm', 'realliampayne', 'realjaxclone', 'realizations', 'realization', 'realities', 'realistic', 'realhotcullen', 'realhiphop', 'reagans', 'reagan', 'reafs', 'readiness', 'reader', 'reacts', 'reactors', 'reactorbased', 'reactions', 'reached', 'reaad', 'rdg', 'rdconsider', 'rconspiracy', 'rcityporn', 'rchs', 'rbi', 'rbcinsurance', 'razedåÊ\\x89ÛÒ', 'razak', 'rayquazaerk', 'raychielovesu', 'rawfoodbliss', 'ravioliåÊwith', 'rave', 'rationing', 'ratio', 'ratingscategories', 'ratings', 'ratingbut', 'rascal', 'raredealsuk', 'rar', 'raptorsbeg', 'raped', 'rants', 'rantipozi', 'ransacked', 'ranks', 'ranking', 'ranked', 'rank', 'raniakhalek', 'rangerkaitimay', 'rang', 'randy', 'randomtourist', 'randomthought', 'randerson62', 'randallpinkston', 'rams', 'rampage', 'ramp', 'ramat', 'ram', 'ralph', 'rally\\x89Û', 'raisinfingers', 'raishimi33', 'rainy', 'rainwindstorm', 'rainforestresq', 'raineishida', 'railroad', 'railguns', 'raidersreporter', 'rahulkanwal', 'raheelsharif', 'raheel', 'raging', 'rag', 'raft', 'raffirc', 'radychildrens', 'radler', 'radios', 'radioriffrocks', 'radical', 'rachelcaine', 'racer', 'raccoons', 'racco', 'rabidmonkeys1', 'rabbit', 'rabaa', 'raabchar28', 'ra', 'r5live', 'r3do', 'r21', 'r1354', 'qzloremft', 'qz', 'quottelevision', 'quotoperations', 'quotesttg', 'quoteoftheday', 'quora', 'quizzed', 'quit', 'quirk', 'quiet', 'quicker', 'quests', 'questionfatalityflawless', 'questergirl', 'quem', 'queer', 'queenwendy', 'queenswharf', 'queenmy', 'quarter', 'quarrel', 'quantit\\x89Ûhttpstco64cymg1ltg', 'quals', 'qualit', 'quake', 'quadrillion', 'qty', 'qpr1980', 'qotring', 'qnh', 'qew', 'qendil', 'qave', 'qampa', 'q99', 'q13', 'q1', 'python', 'pyrotechnic', 'pyrbliss', 'pyramidhead76', 'pydisney', 'pwhvgwax', 'pvris', 'puts', 'puth', 'pussyxdestroyer', 'pusssssssssy', 'push2left', 'purposely', 'purpose', 'purported', 'purpleturtlerdg', 'purified', 'purely', 'purdies', 'puppyshogun', 'puppet', 'pup', 'puny', 'punk', 'punishing', 'punishable', 'pundits', 'pundit', 'punch', 'pumpkin', 'pumper', 'pumped', 'pummel', 'pulse', 'pull\\x89Ûone\\x89Ûyou', 'pullup', 'pulkovo', 'puledotechupdate', 'pugwash', 'pugprobs', 'pug', 'puff', 'puerto', 'puddle', 'puckflattened', 'publishing', 'publish', 'publicityalthough', 'publichealth', 'pub', 'pu', 'pt4', 'pt1', 'psychrewatch', 'psychologist', 'psychic', 'psp', 'psfda', 'pseudojuuzo', 'psd', 'psalm3422', 'psa', 'ps3', 'ps2', 'ps1', 'prysmian', 'proxy', 'proxies', 'provokes', 'providers', 'provided', 'proven', 'prove', 'proudgreenhome', 'protostates', 'protestors', 'protesters', 'protein', 'protected', 'prosser', 'prosper', 'pros', 'proportions', 'prophets', 'prophecy', 'propertycasu', 'properly', 'propelled', 'propane', 'propaganda', 'pronouncing', 'proms', 'prompting', 'prompt', 'promotion', 'promoted', 'promo', 'promised', 'prom', 'prolong', 'prolly', 'proliferation', 'projectiles\\x89ÛÒ', 'progressives', 'progress4ohio', 'programs', 'profittothepeople', 'profithungry', 'professionally', 'profbriancox', 'productive', 'production', 'produces', 'producer', 'produce', 'produc', 'prodemocracy', 'proc', 'probs', 'probability', 'prob', 'privilege', 'prisonplanet', 'prisoners', 'prints', 'printing', 'printed', 'printable', 'principle', 'princessduck', 'princeoffencing', 'primalkitchen', 'primal', 'priests', 'priest', 'pride', 'prez', 'previews', 'preventative', 'prevalent', 'prettyboyshyflizzy', 'pretenses', 'presume', 'preston\\x89Ûªs', 'prestige', 'presstv', 'presssec', 'pressing', 'presser', 'presley', 'presinkhole', 'president\\x89Û\\x9d', 'presidential', 'preset', 'preserve', 'presents', 'presentation', 'preseasonworkouts', 'preseason', 'preschool', 'presbad', 'preppertalk', 'preppers', 'preparedelectrocutedboiling', 'prensa', 'premises', 'prem', 'preferable', 'prefecture', 'preemptive', 'predynastic', 'predictions', 'preconditioning', 'precisionistic', 'precedent', 'preaching', 'preacher', 'praying', 'prayforsaipan', 'prayed', 'praise', 'pragnik', 'practitioner', 'practicing', 'practicenyg', 'practically', 'prablematicla', 'pra', 'ppsellsbabyparts', 'ppor', 'pple', 'ppfa', 'ppc', 'ppact', 'pp400dr', 'pp15000266858', 'pp15000266818', 'pp', 'poze', 'pozarmy', 'pox', 'powerwow', 'powers', 'powerhiroshima', 'powder', 'poway', 'pow', 'pouring', 'poured', 'pour', 'pounds', 'pounding', 'pounded', 'pouch', 'potter', 'pots', 'potatoes', 'pot', 'postponed', 'postexistence', 'postering', 'poster', 'postcards', 'postcapitalism', 'postal', 'possess', 'possesion', 'positively', 'poses', 'pos', 'portrait', 'portfolio', 'portaloos', 'porno', 'pornhub', 'porcupine', 'porcini', 'population6', 'popeyes', 'popcorn', 'pop2015', 'pony', 'ponting', 'pone', 'polluted', 'pollster', 'politic\\x89Û', 'politicized', 'politicians', 'politely', 'polit', 'policylab', 'police\\x89Û', 'policeng', 'pole', 'polaroids', 'polar', 'pol', 'pokemoncards', 'pokemon', 'poisoned', 'point\\x89Û', 'pointless', 'poignant', 'pogo', 'podcast', 'pod', 'poconorecord', 'pochette', 'poc', 'pneumonia', 'plymouth', 'plumbing', 'plugin', 'plsss', 'plotted', 'ploppy', 'pll', 'plez', 'pletch\\x89Ûªs', 'pledged', 'pleb', 'pleasant', 'pleaded', 'plaza', 'playthrough', 'playstation', 'playoverwatch', 'playingnow', 'playa', 'platt', 'platinum', 'plastics', 'plastic', 'plantcovered', 'plantations', 'planners', 'plannedparenthood', 'plank', 'planing', 'planetary', 'pl', 'pkwy', 'pjcoyle', 'pizzas', 'pizzarev', 'pixelsmovie', 'pixeljanosz', 'pixelcanuck', 'pixar', 'pivot', 'pity', 'pittsburgh', 'pitmix', 'pitcher', 'pitched', 'pit', 'piss', 'pisco', 'pirates', 'pirate', 'piracy', 'pir', 'piprhys', 'piping', 'piperwearsthepants', 'pipeliners', 'pioneer', 'pineview', 'pin23928835', 'pills', 'pilgrims', 'pileup', 'piles', 'pileq', 'pikin', 'pikachu', 'pigeon', 'piga', 'piercings', 'piercing', 'pierce', 'pierc', 'piece\\x89Û', 'pieceofme', 'pictwittercompnpizody', 'pictured', 'picthis', 'pickpocket', 'pickle', 'pickens', 'pianohands', 'piano', 'physician', 'phuket', 'photoop', 'photographs', 'photographed', 'photogenic', 'phnotf', 'phillips', 'phillip', 'philippines\\x89Û', 'philippine', 'philippi', 'philipduncan', 'philip', 'phelimkine', 'pharrell', 'pharma', 'phantasmal', 'phandom', 'phalaborwa', 'ph0tos', 'pga', 'pft', 'pfft', 'pfannebeckers', 'petty', 'petting', 'petitiontake', 'petitionno', 'petersens', 'petersburg', 'peterknox', 'peterhowenecn', 'petereallen', 'peterduttonmp', 'petelmcguire', 'petebests', 'pete', 'petchary', 'peta', 'pet', 'pestle', 'perspectives', 'perspective', 'personnel', 'personalize', 'personalinjury', 'persistent', 'persist', 'perrychat', 'perrybellegarde', 'perrie', 'perpetrators', 'permission', 'periwinkle', 'perished', 'periscope', 'perform', 'perforated', 'perfectly', 'perceive', 'pepperoni', 'peoplegt', 'peoplecommunication', 'pension', 'penny', 'pennlive', 'pennies', 'penneys', 'peninsula', 'penetrate', 'penalty', 'pelosis', 'peice', 'peeters', 'peers', 'peeped', 'peel', 'peeked', 'pee', 'pedro20', 'pediatric', 'pedals', 'peasants', 'pearlharbor', 'pearl', 'peale', 'peace\\x89Ûª', 'peacetime', 'pdxabq', 'pd', 'pci', 'pcaldicott7', 'pbx', 'pbs', 'pbohanna', 'pbcanpcx', 'pb', 'paypile', 'payment', 'paying', 'paydayprison', 'payday', 'payback', 'paxton', 'pawsox', 'paws', 'paulstaubs', 'pauls', 'paulista', 'paulhollywood', 'pattyds50', 'patterns', 'patrol', 'patrickwsls', 'patrickjbutler', 'patriciatraina', 'patna', 'patio', 'patientreported', 'paths', 'pathfinders', 'pat', 'pastures', 'pastor', 'pastie', 'passive', 'passion', 'pascoe', 'pascal', 'partys', 'partners', 'partner', 'parties', 'participating', 'participate', 'partially', 'parter', 'partake', 'parsholics', 'pars', 'parliment', 'parliamentary', 'parley\\x89Ûªs', 'parksboardfacts', 'parks', 'parked', 'parkchat', 'parisian', 'parental', 'parent', 'pardon', 'parched', 'paratroopers', 'paramore', 'paramedics', 'paraguay', 'paradise', 'paracord', 'para', 'papicongress', 'papi', 'paperwork', 'paperback', 'papcrdoll', 'papa', 'pantofel', 'panties', 'panther', 'pantalonesfuego', 'panik', 'panics', 'panicked', 'pandora', 'pandemoniumiso', 'pandemic', 'pancakes', 'panama', 'pampered', 'pampalmater', 'palmoil', 'palmer', 'palm', 'palinfoen', 'palestinian\\x89Û', 'palermo', 'paleface', 'pale', 'pakthey', 'pakistans', 'pajamas', 'paints', 'painthey', 'painful', 'paine', 'paging', 'pageshi', 'pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n",
      "\n",
      "\n",
      "10000\n",
      "\n",
      "\n",
      "['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layers\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "print(words_in_vocab)\n",
    "print('\\n')\n",
    "print(len(words_in_vocab))\n",
    "print(\"\\n\")\n",
    "print(words_in_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79d7d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75d7d3",
   "metadata": {},
   "source": [
    "Get the weight matrix of our embedding layer\n",
    "these are the numerical representations of each token in our training data, which have been learned for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da15505f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[ 0.00073164,  0.01504801, -0.03425453, ..., -0.04403543,\n",
       "         -0.01042281,  0.01876438],\n",
       "        [ 0.04135863, -0.03945085, -0.03811941, ...,  0.00464735,\n",
       "          0.03163552,  0.02928301],\n",
       "        [ 0.00684031,  0.05363135, -0.00241554, ..., -0.07082177,\n",
       "         -0.04750703,  0.01448254],\n",
       "        ...,\n",
       "        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n",
       "          0.00308807,  0.02215792],\n",
       "        [ 0.00692343,  0.05942352, -0.01975194, ..., -0.06199061,\n",
       "         -0.01018393,  0.03510419],\n",
       "        [-0.0372346 ,  0.06267187, -0.07451147, ..., -0.02367218,\n",
       "         -0.08643329,  0.01742155]], dtype=float32)>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61458ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shapes of the first embedding\n",
    "\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "embed_weights.shape # same size as the vocab size and embedding dim(output_dim of our embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1525602",
   "metadata": {},
   "source": [
    "The (10000, 128) above means that every token above is represented by a vector of 128 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2eea31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding files (we got this from tensorflow word embedding documentation)\n",
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = embed_weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6096344",
   "metadata": {},
   "source": [
    "nce you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:\n",
    "- Go to http://projector.tensorflow.org/\n",
    "- Click on \"Load data\"\n",
    "- Upload the two files you downloaded (embedding_vectors.tsv and embedding_metadata.tsv)\n",
    "- Explore Optional: You can share the data you've created by clicking \"Publish\"\n",
    "\n",
    "The embeddings we downloaded are how our model interprets words, not necessarily how we interpret them.\n",
    "\n",
    "Also, since the embedding has been learned purely from Tweets, it may contain some strange values as Tweets are a very unique style of natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255a970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d13aa1ce",
   "metadata": {},
   "source": [
    "# Model 2 : LSTM- Long Short Term Memory (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81e4b4",
   "metadata": {},
   "source": [
    "We're going to start with an LSTM-powered RNN.\n",
    "\n",
    "To harness the power of the LSTM cell (LSTM cell and LSTM layer are often used interchangably) in TensorFlow, we'll use tensorflow.keras.layers.LSTM().\n",
    "\n",
    "Our model is going to take on a very similar structure to model_1:\n",
    "\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "\n",
    "The main difference will be that we're going to add an LSTM layer between our embedding and output.\n",
    "\n",
    "And to make sure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (model_2_embedding) for our model. The text_vectorizer layer can be reused since it doesn't get updated during training.\n",
    "\n",
    "🔑 Note: The reason we use a new embedding layer for each model is since the embedding layer is a learned representation of words (as numbers), if we were to use the same embedding layer (embedding_1) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fcbddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed and create new embedding layer for this model\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                                    output_dim = 128,\n",
    "                                    embeddings_initializer = \"uniform\",\n",
    "                                    input_length = max_length,\n",
    "                                    name = \"embedding_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6306e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "inputs = layers.Input(shape=(1,), dtype = \"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "\n",
    "#print(x.shape)\n",
    "# x = layers.LSTM(return_sequences = True) (x) # when we are stacking RNN cells together\n",
    "# print(x.shape)\n",
    "\n",
    "x = layers.LSTM(64)(x)\n",
    "# print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "# print(x.shape)\n",
    "\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name = \"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cdba099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the model summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3accc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = tf.keras.optimizers.Adam(),\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9657cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/model_2_LSTM/20220112-161418\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 14ms/step - loss: 0.5100 - accuracy: 0.7415 - val_loss: 0.4567 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8747 - val_accuracy: 0.7507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14032aca2c8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_2.fit(train_sentences, \n",
    "           train_labels,\n",
    "           epochs = 5,\n",
    "           validation_data = (val_sentences, val_labels),\n",
    "           callbacks = [create_tensorboard_callback(SAVE_DIR,\n",
    "                                                   \"model_2_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b936960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00713337],\n",
       "       [0.7874562 ],\n",
       "       [0.9996377 ],\n",
       "       [0.05690471],\n",
       "       [0.00258272],\n",
       "       [0.9996238 ],\n",
       "       [0.9216488 ],\n",
       "       [0.9997993 ],\n",
       "       [0.9994956 ],\n",
       "       [0.66466784]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictins with LSTM Model\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f11ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Model 2 pred probs to labels\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0b3d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.06561679790026,\n",
       " 'precision': 0.7510077975908164,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1': 0.7489268622514025}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate performance metrics\n",
    "model_2_results = calculate_results(y_true = val_labels,\n",
    "                                   y_pred = model_2_preds)\n",
    "\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1200a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n",
      "Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n",
      "Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n",
      "Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"
     ]
    }
   ],
   "source": [
    "# Compare these results with baseline results\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c1e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c273033",
   "metadata": {},
   "source": [
    "# Model 3: GRU (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1126",
   "metadata": {},
   "source": [
    "Another popular and effective RNN component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters.\n",
    "\n",
    "To use the GRU cell in TensorFlow, we can call the tensorflow.keras.layers.GRU() class.\n",
    "\n",
    "The architecture of the GRU-powered model will follow the same structure we've been using:\n",
    "\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "\n",
    "Again, the only difference will be the layer(s) we use between the embedding and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba411477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new embedding for model 3\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model_3_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                                    output_dim = 128,\n",
    "                                    embeddings_initializer = \"uniform\",\n",
    "                                    input_length = max_length,\n",
    "                                    name = \"embedding_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a257f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an RNN using GRU cell\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "\n",
    "# x = layers.GRU(64, return_sequences=True)\n",
    "# x = layers.LSTM(42, return_sequences = True)\n",
    "# x = layers.GRU(99)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3858827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_3.compile(loss = \"binary_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c14b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the GRU model\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa940",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice the difference in number of trainable parameters between model_2 (LSTM) and model_3 (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell.\n",
    "\n",
    "We'll fit our model just as we've been doing previously. We'll also track our models results using our create_tensorboard_callback() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecf19e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/model_3_GRU/20220112-161435\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 15ms/step - loss: 0.5242 - accuracy: 0.7314 - val_loss: 0.4553 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.3195 - accuracy: 0.8695 - val_loss: 0.4937 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2197 - accuracy: 0.9181 - val_loss: 0.5607 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1599 - accuracy: 0.9441 - val_loss: 0.6220 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1221 - accuracy: 0.9584 - val_loss: 0.6205 - val_accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "# Fit the Model\n",
    "model_3_history = model_3.fit(train_sentences, \n",
    "                             train_labels,\n",
    "                             epochs = 5,\n",
    "                             validation_data = (val_sentences, val_labels),\n",
    "                             callbacks = [create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     experiment_name = \"model_3_GRU\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2486b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3333033 ],\n",
       "       [0.8774384 ],\n",
       "       [0.9980247 ],\n",
       "       [0.11559197],\n",
       "       [0.01236264],\n",
       "       [0.9925619 ],\n",
       "       [0.6213533 ],\n",
       "       [0.9981324 ],\n",
       "       [0.998237  ],\n",
       "       [0.5017956 ]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation data\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "print(model_3_pred_probs.shape)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40f00755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert probabilities to prediction classes\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b9036f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7675450859410361,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.7667932666650168}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model3 results\n",
    "model_3_results = calculate_results(y_true = val_labels,\n",
    "                                   y_pred = model_3_preds)\n",
    "\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e95a3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# compare to the baseline\n",
    "compare_baseline_to_new_results (baseline_results,\n",
    "                                model_3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4aabc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89be2f77",
   "metadata": {},
   "source": [
    "# Model 4 : Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487091d",
   "metadata": {},
   "source": [
    "Look at us go! We've already built two RNN's with GRU and LSTM cells. Now we're going to look into another kind of RNN, the bidirectional RNN.\n",
    "\n",
    "A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.\n",
    "\n",
    "Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).\n",
    "\n",
    "In practice, many sequence models often see and improvement in performance when using bidirectional RNN's.\n",
    "\n",
    "However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles).\n",
    "\n",
    "Okay enough talk, let's build a bidirectional RNN.\n",
    "\n",
    "Once again, TensorFlow helps us out by providing the tensorflow.keras.layers.Bidirectional class. We can use the Bidirectional class to wrap our existing RNNs, instantly making them bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57122609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer for model 4\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_4_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                                    output_dim = 128,\n",
    "                                    embeddings_initializer = \"uniform\",\n",
    "                                    input_length = max_length,\n",
    "                                    name = \"embedding_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6473c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bi-directional RNN Model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_4_embedding(x)\n",
    "\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences = True)) # when we want to stack RNN layers on top of each other\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"Model_4_Bidirectional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5ffee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the model summary\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "072b3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_4.compile(loss = \"binary_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4606871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/model_4_bidirectional_RNN/20220112-161451\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 22ms/step - loss: 0.5093 - accuracy: 0.7481 - val_loss: 0.4606 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.5144 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.5626 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1523 - accuracy: 0.9469 - val_loss: 0.6365 - val_accuracy: 0.7769\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.6510 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Fit the Model\n",
    "model_4_history= model_4.fit(train_sentences, \n",
    "           train_labels,\n",
    "           epochs = 5,\n",
    "           validation_data = (val_sentences, val_labels),\n",
    "           callbacks = [create_tensorboard_callback(SAVE_DIR,\n",
    "                                                   experiment_name = \"model_4_bidirectional_RNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "adad5b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03995152],\n",
       "       [0.82796067],\n",
       "       [0.99842024],\n",
       "       [0.13519038],\n",
       "       [0.00310786],\n",
       "       [0.9921989 ],\n",
       "       [0.95532155],\n",
       "       [0.9994561 ],\n",
       "       [0.9989818 ],\n",
       "       [0.28122008]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions with Bidirectional Model\n",
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f61dbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert pred probs to pred labels\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbee2f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7665895370389821,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.7651213533864446}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the results of the Bidirectional Model\n",
    "model_4_results = calculate_results(y_true = val_labels,\n",
    "                                   y_pred = model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e74cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.03\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "# Check to see how the bidirectional model performs against the baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_4_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbd252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973441e7",
   "metadata": {},
   "source": [
    "# Model 5 : Conv1D for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82924900",
   "metadata": {},
   "source": [
    "You might've used convolutional neural networks (CNNs) for images before but they can also be used for sequences.\n",
    "\n",
    "The main difference between using CNNs for images and sequences is the shape of the data. Images come in 2-dimensions (height x width) where as sequences are often 1-dimensional (a string of text).\n",
    "\n",
    "So to use CNNs with sequences, we use a 1-dimensional convolution instead of a 2-dimensional convolution.\n",
    "\n",
    "A typical CNN architecture for sequences will look like the following:\n",
    "\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layers -> Outputs (class probabilities)\n",
    "\n",
    "You might be thinking \"that just looks like the architecture layout we've been using for the other models...\"\n",
    "\n",
    "And you'd be right.\n",
    "\n",
    "The difference again is in the layers component. Instead of using an LSTM or GRU cell, we're going to use a tensorflow.keras.layers.Conv1D() layer followed by a tensorflow.keras.layers.GlobablMaxPool1D() layer.\n",
    "\n",
    "    📖 Resource: The intuition here is explained succinctly in the paper Understanding Convolutional Neural Networks for Text Classification, where they state that CNNs classify text through the following steps:\n",
    "\n",
    "- dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in \"hello, my name is Daniel\").\n",
    "- Max-pooling over time extracts the relevant ngrams for making a decision.\n",
    "- The rest of the network classifies the text based on this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7a595ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 128)\n",
      "(1, 15, 32)\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "# Test out oue Embedding layer, conv1d layer and max pooling layer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sequence to embedding\n",
    "conv_1d = layers.Conv1D(filters = 32,\n",
    "                       kernel_size =5, # this is also referred to as an ngram of 5 (meaning it looks at 5 words at a time)\n",
    "                       activation = \"relu\",\n",
    "                       padding = \"same\") # defalt = \"valid\", the output is smaller than the input shape. \"same\" means output shape is the same as th input \n",
    "conv_1d_output = conv_1d(embedding_test) # pass the embedding through conv1d layer\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # equivalent to \"get the most important featuer\" or \"get the feature with the highest value\"\n",
    "\n",
    "print(embedding_test.shape)\n",
    "print(conv_1d_output.shape)\n",
    "print(max_pool_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe2ae2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice the output shapes of each layer.\n",
    "\n",
    "The embedding has an output shape dimension of the parameters we set it to (input_length=15 and output_dim=128).\n",
    "\n",
    "The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.\n",
    "\n",
    "Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).\n",
    "\n",
    "Let's take a peak at what each of these transformations looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1bc64855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       " array([[[ 0.02534914, -0.03109059,  0.00285616, ..., -0.0078316 ,\n",
       "          -0.02685576, -0.04434131],\n",
       "         [-0.0658626 ,  0.09451494, -0.01477602, ..., -0.00657781,\n",
       "          -0.0423879 ,  0.07777896],\n",
       "         [-0.04803652, -0.00709757, -0.02330893, ..., -0.0180733 ,\n",
       "           0.02351034,  0.02676384],\n",
       "         ...,\n",
       "         [ 0.00073164,  0.01504801, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042281,  0.01876438],\n",
       "         [ 0.00073164,  0.01504801, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042281,  0.01876438],\n",
       "         [ 0.00073164,  0.01504801, -0.03425453, ..., -0.04403543,\n",
       "          -0.01042281,  0.01876438]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 15, 32), dtype=float32, numpy=\n",
       " array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.00736916, 0.        , 0.01438061, 0.03620387, 0.        ,\n",
       "          0.0286228 , 0.0236714 , 0.        , 0.03180662, 0.06328546,\n",
       "          0.07848544, 0.        , 0.03581524, 0.0768801 , 0.        ,\n",
       "          0.        , 0.        , 0.05471281, 0.        , 0.        ,\n",
       "          0.00177272, 0.        , 0.06187331, 0.        , 0.        ,\n",
       "          0.01044424, 0.        ],\n",
       "         [0.09494424, 0.        , 0.06744662, 0.03645384, 0.07902624,\n",
       "          0.02401439, 0.00555635, 0.        , 0.0262702 , 0.        ,\n",
       "          0.11972354, 0.        , 0.        , 0.01816024, 0.00963105,\n",
       "          0.        , 0.        , 0.        , 0.10587421, 0.        ,\n",
       "          0.06144584, 0.0273056 , 0.10417719, 0.        , 0.        ,\n",
       "          0.01986323, 0.04072177, 0.        , 0.        , 0.        ,\n",
       "          0.05649755, 0.        ],\n",
       "         [0.08324981, 0.00648717, 0.        , 0.03983569, 0.        ,\n",
       "          0.01144417, 0.00416251, 0.02288389, 0.        , 0.00900978,\n",
       "          0.        , 0.        , 0.0340177 , 0.06408273, 0.08103719,\n",
       "          0.00409015, 0.01579618, 0.        , 0.07930179, 0.        ,\n",
       "          0.        , 0.        , 0.14525084, 0.        , 0.        ,\n",
       "          0.        , 0.03682076, 0.06534286, 0.        , 0.        ,\n",
       "          0.05094622, 0.        ],\n",
       "         [0.        , 0.05387189, 0.        , 0.11491334, 0.        ,\n",
       "          0.        , 0.16237083, 0.        , 0.        , 0.00171254,\n",
       "          0.14336711, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01197934, 0.        , 0.        , 0.13551371, 0.00401058,\n",
       "          0.10309823, 0.09445539, 0.08390294, 0.        , 0.04213035,\n",
       "          0.04487598, 0.06560458, 0.        , 0.02272682, 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.03683224, 0.04895766, 0.        , 0.15324757, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.04650313, 0.00496457, 0.07349403, 0.0160864 ,\n",
       "          0.        , 0.02779122, 0.        , 0.08080561, 0.01403177,\n",
       "          0.        , 0.03768811, 0.1038278 , 0.        , 0.03361667,\n",
       "          0.        , 0.02577604, 0.00140355, 0.        , 0.        ,\n",
       "          0.032115  , 0.        ],\n",
       "         [0.00887822, 0.10450976, 0.        , 0.06974537, 0.02328688,\n",
       "          0.        , 0.04052211, 0.        , 0.        , 0.02733763,\n",
       "          0.08674344, 0.        , 0.        , 0.06129852, 0.02007267,\n",
       "          0.        , 0.        , 0.        , 0.03364265, 0.        ,\n",
       "          0.04525332, 0.05219701, 0.06375708, 0.        , 0.        ,\n",
       "          0.00774407, 0.00273467, 0.        , 0.        , 0.00499632,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.0236907 , 0.        , 0.05827619, 0.05297642,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.01719715, 0.0293682 , 0.00466101, 0.06879886, 0.01944804,\n",
       "          0.01585533, 0.01294549, 0.        , 0.06866527, 0.        ,\n",
       "          0.00623768, 0.03514049, 0.02407536, 0.        , 0.05979817,\n",
       "          0.        , 0.01170142, 0.        , 0.        , 0.        ,\n",
       "          0.04444931, 0.        ],\n",
       "         [0.03544857, 0.        , 0.        , 0.05054974, 0.06105438,\n",
       "          0.        , 0.00997432, 0.0140301 , 0.        , 0.01680729,\n",
       "          0.03148504, 0.03889387, 0.        , 0.07710677, 0.00590968,\n",
       "          0.        , 0.00263036, 0.        , 0.08935823, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227952, 0.        , 0.06658386,\n",
       "          0.01881708, 0.02448697, 0.        , 0.        , 0.        ,\n",
       "          0.0200846 , 0.        ],\n",
       "         [0.03544859, 0.        , 0.        , 0.05054974, 0.06105439,\n",
       "          0.        , 0.00997431, 0.01403011, 0.        , 0.01680729,\n",
       "          0.03148504, 0.03889388, 0.        , 0.07710679, 0.00590968,\n",
       "          0.        , 0.00263037, 0.        , 0.08935825, 0.        ,\n",
       "          0.        , 0.05331149, 0.05227952, 0.        , 0.06658387,\n",
       "          0.01881708, 0.02448697, 0.        , 0.        , 0.        ,\n",
       "          0.0200846 , 0.        ],\n",
       "         [0.03544858, 0.        , 0.        , 0.05054974, 0.06105439,\n",
       "          0.        , 0.00997431, 0.01403011, 0.        , 0.01680729,\n",
       "          0.03148504, 0.03889387, 0.        , 0.07710678, 0.00590969,\n",
       "          0.        , 0.00263037, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331149, 0.05227951, 0.        , 0.06658385,\n",
       "          0.01881708, 0.02448697, 0.        , 0.        , 0.        ,\n",
       "          0.02008461, 0.        ],\n",
       "         [0.03544859, 0.        , 0.        , 0.05054973, 0.06105439,\n",
       "          0.        , 0.0099743 , 0.0140301 , 0.        , 0.0168073 ,\n",
       "          0.03148503, 0.03889387, 0.        , 0.07710677, 0.00590968,\n",
       "          0.        , 0.00263037, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331148, 0.05227952, 0.        , 0.06658387,\n",
       "          0.01881708, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.0200846 , 0.        ],\n",
       "         [0.0354486 , 0.        , 0.        , 0.05054974, 0.06105439,\n",
       "          0.        , 0.00997431, 0.0140301 , 0.        , 0.01680729,\n",
       "          0.03148504, 0.03889388, 0.        , 0.07710677, 0.00590968,\n",
       "          0.        , 0.00263036, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331149, 0.0522795 , 0.        , 0.06658387,\n",
       "          0.01881708, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.0200846 , 0.        ],\n",
       "         [0.0354486 , 0.        , 0.        , 0.05054974, 0.06105439,\n",
       "          0.        , 0.0099743 , 0.0140301 , 0.        , 0.01680729,\n",
       "          0.03148504, 0.03889388, 0.        , 0.07710678, 0.00590968,\n",
       "          0.        , 0.00263037, 0.        , 0.08935824, 0.        ,\n",
       "          0.        , 0.05331149, 0.05227952, 0.        , 0.06658387,\n",
       "          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n",
       "          0.02008461, 0.        ],\n",
       "         [0.04974463, 0.        , 0.        , 0.03846077, 0.03860972,\n",
       "          0.        , 0.0122603 , 0.01917104, 0.        , 0.02291363,\n",
       "          0.02792527, 0.02171171, 0.        , 0.07409465, 0.011945  ,\n",
       "          0.        , 0.0174341 , 0.        , 0.07744252, 0.        ,\n",
       "          0.        , 0.06878704, 0.05294931, 0.        , 0.04103798,\n",
       "          0.04976272, 0.01104268, 0.        , 0.        , 0.        ,\n",
       "          0.02790449, 0.        ],\n",
       "         [0.04075229, 0.01304907, 0.        , 0.00898046, 0.04526407,\n",
       "          0.        , 0.01434541, 0.01440299, 0.        , 0.02508792,\n",
       "          0.02119408, 0.00345627, 0.        , 0.04556926, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.06290772, 0.00215335,\n",
       "          0.01903836, 0.06594219, 0.01725418, 0.        , 0.07116794,\n",
       "          0.06687687, 0.04677387, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.09494424, 0.10450976, 0.06744662, 0.15324757, 0.07902624,\n",
       "         0.02401439, 0.16237083, 0.02288389, 0.03620387, 0.02733763,\n",
       "         0.14336711, 0.04650313, 0.0340177 , 0.07710679, 0.08103719,\n",
       "         0.07848544, 0.02779122, 0.03581524, 0.13551371, 0.01403177,\n",
       "         0.10309823, 0.09445539, 0.14525084, 0.        , 0.07116794,\n",
       "         0.06687687, 0.06560458, 0.06534286, 0.02272682, 0.00499632,\n",
       "         0.05649755, 0.        ]], dtype=float32)>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "080200cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer for Conv1d model\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model_5_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                                    output_dim = 128,\n",
    "                                    embeddings_initializer = \"uniform\",\n",
    "                                    input_length = max_length,\n",
    "                                    name = \"embedding_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d696da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1D conv layer to model sequence\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  activation=\"relu\",\n",
    "                  padding=\"valid\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\") (x) # optinal dense layer\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# compile the model\n",
    "model_5.compile(loss = \"binary_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2637cb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the model summary\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4567358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/model_5_conv1D/20220112-161516\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 9ms/step - loss: 0.5559 - accuracy: 0.7214 - val_loss: 0.4665 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.3323 - accuracy: 0.8644 - val_loss: 0.4799 - val_accuracy: 0.7822\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2038 - accuracy: 0.9225 - val_loss: 0.5586 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1280 - accuracy: 0.9591 - val_loss: 0.6447 - val_accuracy: 0.7677\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.0901 - accuracy: 0.9696 - val_loss: 0.7039 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                             train_labels,\n",
    "                             epochs = 5,\n",
    "                             validation_data = (val_sentences, val_labels),\n",
    "                             callbacks = [create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                      \"model_5_conv1D\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d055c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step - loss: 0.7039 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7038962841033936, 0.7887139320373535]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68fc6ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.543736  ],\n",
       "       [0.8092995 ],\n",
       "       [0.9998087 ],\n",
       "       [0.03838049],\n",
       "       [0.00113677],\n",
       "       [0.99150544],\n",
       "       [0.9677432 ],\n",
       "       [0.998509  ],\n",
       "       [0.99936634],\n",
       "       [0.28771225]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc850327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert probabilities to classes\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "291afb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.87139107611549,\n",
       " 'precision': 0.7926581572076621,\n",
       " 'recall': 0.7887139107611548,\n",
       " 'f1': 0.7860944810879305}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the model performace metrics\n",
    "model_5_results = calculate_results(y_true = val_labels,\n",
    "                                   y_pred = model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70045b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.87, Difference: -0.39\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.00\n",
      "Baseline f1: 0.79, New f1: 0.79, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "# compare model_5 results to the baseline model\n",
    "compare_baseline_to_new_results(baseline_results, \n",
    "                       model_5_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596d796",
   "metadata": {},
   "source": [
    "# Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4172a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# create directory to save Tensorboard logs\n",
    "SAVE_DIR = \"model_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ef45a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"There's a flood in my street!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56e7905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157027  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n",
      "  0.02680985  0.05589838 -0.01068729 -0.00597292  0.00639323 -0.0181952\n",
      "  0.00030814  0.09105888  0.05874645 -0.03180628  0.01512474 -0.05162929\n",
      "  0.00991367 -0.06865346 -0.04209305  0.0267898   0.03011008  0.00321069\n",
      " -0.00337971 -0.04787356  0.02266719 -0.00985925 -0.04063613 -0.01292093\n",
      " -0.04666384  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014441\n",
      "  0.02871508  0.04947684 -0.00633978 -0.08960193  0.02807117 -0.00808362\n",
      " -0.01360601  0.0599865  -0.10361787 -0.05195374  0.00232955 -0.0233253\n",
      " -0.03758105  0.03327729], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"when you call the Universal Senetence Encoder on a sentence, it turns into numbers\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93dcddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5ef7ef18",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11860/1473737087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# data type of inputs coming to the USE layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# keep the pretrained weights (we'll create a feature extractor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                         name=\"USE\") \n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_is_hub_module_v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Expected before TF2.4.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m   \"\"\"\n\u001b[1;32m--> 900\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    937\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m--> 939\u001b[1;33m                             ckpt_options, options, filters)\n\u001b[0m\u001b[0;32m    940\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msave_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_skip_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCapturableResource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mload_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         options=options)\n\u001b[0;32m   1366\u001b[0m     base.CheckpointPosition(\n\u001b[1;32m-> 1367\u001b[1;33m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[1;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    982\u001b[0m     restore_ops.extend(\n\u001b[0;32m    983\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[1;32m--> 984\u001b[1;33m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[0;32m    985\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    328\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[0;32m    329\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[1;32m--> 330\u001b[1;33m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[0;32m    331\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    321\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m           \u001b[0mrestore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    114\u001b[0m                                           structured_restored_tensors):\n\u001b[0;32m    115\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[1;32m--> 116\u001b[1;33m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msave_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_save_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_var_device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m       \u001b[0mrestored_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[0;32m    134\u001b[0m           self.handle_op, self._var_shape, restored_tensor)\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7105\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7107\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
     ]
    }
   ],
   "source": [
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322035b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
